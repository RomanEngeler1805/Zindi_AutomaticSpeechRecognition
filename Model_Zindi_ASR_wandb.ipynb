{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf21f168",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e4c9de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## path\n",
    "path = '/home/andrschl/Documents/projects/Zindi_AutomaticSpeechRecognition/'\n",
    "# path = 'drive/MyDrive/Colab Notebooks/'\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7572ea34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /home/andrschl/.virtualenvs/zindi/lib/python3.8/site-packages (1.5.0)\n",
      "Requirement already satisfied: xxhash in /home/andrschl/.virtualenvs/zindi/lib/python3.8/site-packages (from datasets) (2.0.0)\n",
      "Requirement already satisfied: fsspec in /home/andrschl/.virtualenvs/zindi/lib/python3.8/site-packages (from datasets) (0.8.7)\n",
      "Requirement already satisfied: pyarrow>=0.17.1 in /home/andrschl/.virtualenvs/zindi/lib/python3.8/site-packages (from datasets) (3.0.0)\n",
      "Requirement already satisfied: dill in /home/andrschl/.virtualenvs/zindi/lib/python3.8/site-packages (from datasets) (0.3.3)\n",
      "Requirement already satisfied: huggingface-hub<0.1.0 in /home/andrschl/.virtualenvs/zindi/lib/python3.8/site-packages (from datasets) (0.0.8)\n",
      "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /home/andrschl/.virtualenvs/zindi/lib/python3.8/site-packages (from datasets) (4.49.0)\n",
      "Requirement already satisfied: multiprocess in /home/andrschl/.virtualenvs/zindi/lib/python3.8/site-packages (from datasets) (0.70.11.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/andrschl/.virtualenvs/zindi/lib/python3.8/site-packages (from datasets) (1.20.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/andrschl/.virtualenvs/zindi/lib/python3.8/site-packages (from datasets) (2.22.0)\n",
      "Requirement already satisfied: pandas in /home/andrschl/.virtualenvs/zindi/lib/python3.8/site-packages (from datasets) (1.2.3)\n",
      "Requirement already satisfied: filelock in /home/andrschl/.virtualenvs/zindi/lib/python3.8/site-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/andrschl/.virtualenvs/zindi/lib/python3.8/site-packages (from pandas->datasets) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/andrschl/.virtualenvs/zindi/lib/python3.8/site-packages (from pandas->datasets) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/andrschl/.virtualenvs/zindi/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.14.0)\n",
      "Collecting git+https://github.com/huggingface/transformers\n",
      "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-qk5gt11a\n",
      "  Running command git clone -q https://github.com/huggingface/transformers /tmp/pip-req-build-qk5gt11a\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied (use --upgrade to upgrade): transformers==4.6.0.dev0 from git+https://github.com/huggingface/transformers in /home/andrschl/.virtualenvs/zindi/lib/python3.8/site-packages\n",
      "Requirement already satisfied: filelock in /home/andrschl/.virtualenvs/zindi/lib/python3.8/site-packages (from transformers==4.6.0.dev0) (3.0.12)\n",
      "Requirement already satisfied: packaging in /home/andrschl/.virtualenvs/zindi/lib/python3.8/site-packages (from transformers==4.6.0.dev0) (20.3)\n",
      "Requirement already satisfied: requests in /home/andrschl/.virtualenvs/zindi/lib/python3.8/site-packages (from transformers==4.6.0.dev0) (2.22.0)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/andrschl/.virtualenvs/zindi/lib/python3.8/site-packages (from transformers==4.6.0.dev0) (0.10.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/andrschl/.virtualenvs/zindi/lib/python3.8/site-packages (from transformers==4.6.0.dev0) (4.49.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.0.8 in /home/andrschl/.virtualenvs/zindi/lib/python3.8/site-packages (from transformers==4.6.0.dev0) (0.0.8)\n",
      "Requirement already satisfied: sacremoses in /home/andrschl/.virtualenvs/zindi/lib/python3.8/site-packages (from transformers==4.6.0.dev0) (0.0.44)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/andrschl/.virtualenvs/zindi/lib/python3.8/site-packages (from transformers==4.6.0.dev0) (2021.3.17)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/andrschl/.virtualenvs/zindi/lib/python3.8/site-packages (from transformers==4.6.0.dev0) (1.20.2)\n",
      "Requirement already satisfied: six in /home/andrschl/.virtualenvs/zindi/lib/python3.8/site-packages (from sacremoses->transformers==4.6.0.dev0) (1.14.0)\n",
      "Requirement already satisfied: click in /home/andrschl/.virtualenvs/zindi/lib/python3.8/site-packages (from sacremoses->transformers==4.6.0.dev0) (7.1.2)\n",
      "Requirement already satisfied: joblib in /home/andrschl/.virtualenvs/zindi/lib/python3.8/site-packages (from sacremoses->transformers==4.6.0.dev0) (1.0.1)\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformers: filename=transformers-4.6.0.dev0-py3-none-any.whl size=2120912 sha256=488699b4a27bcd10edc8f860f9ca7c5780c5033d4a1ae3d025d54e06ac7f3242\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-3l07apdx/wheels/42/68/45/c63edff61c292f2dfd4df4ef6522dcbecc603e7af82813c1d7\n",
      "Successfully built transformers\n"
     ]
    }
   ],
   "source": [
    "# installing\n",
    "!pip install datasets\n",
    "!pip install git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5415dbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load packages\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import os.path\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from torch.utils.data import random_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import librosa as lb\n",
    "\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2CTCTokenizer\n",
    "from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Processor\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a47e0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seeding\n",
    "random.seed(10)\n",
    "np.random.seed(10)\n",
    "torch.manual_seed(10)\n",
    "torch.cuda.manual_seed_all(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10a895ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d535372",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12a6d480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ID', 'up_votes', 'down_votes', 'age', 'gender', 'transcription'], dtype='object')\n",
      "                                                  ID  up_votes  down_votes  \\\n",
      "0  002e50c29ac2890c7cb3b3d63dcbe512bc6850de206ca8...         2           0   \n",
      "1  0031672b4484f963c8a07babe6f713dd559539d44140e8...         2           0   \n",
      "2  00362ccc6b48d3ea225e12ddf8a06a9d582cccc03b0353...         2           0   \n",
      "3  0042cdb4d4a015cddacc26d88faffdd714b7a27213d2b3...         2           0   \n",
      "4  00439f02fa8f0dade934bdb317199b71662e9396f6bb81...         2           0   \n",
      "\n",
      "        age  gender                  transcription  \n",
      "0       NaN     NaN                        Malicka  \n",
      "1  twenties  female  Ecole Elementaire Pikine Nord  \n",
      "2  twenties    male         Cimetière de Cambérène  \n",
      "3  twenties    male                Tournalou Boune  \n",
      "4  twenties    male             Pharmacie Golf Sud  \n"
     ]
    }
   ],
   "source": [
    "# check out dataframe\n",
    "df = pd.read_csv(path + 'data/ASR_train.csv')\n",
    "print(df.keys())\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7f8d3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppress annoying warnings while reading audio files\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb94c303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exist\n"
     ]
    }
   ],
   "source": [
    "## read train set into memory & store as .feather\n",
    "nsamples = len(df)\n",
    "\n",
    "# check if already existent\n",
    "if os.path.isfile(path + 'data/ASR_train_audio'+str(nsamples)+'.ft'):\n",
    "    print (\"File exist\")\n",
    "    df = pd.read_feather(path + 'data/ASR_train_audio'+str(nsamples)+'.ft')\n",
    "else:\n",
    "    print(\"File does not exist\")\n",
    "\n",
    "    # initialize with list\n",
    "    audio_signals = len(df['ID'])*[[0]]\n",
    "    df['audio_signal'] = audio_signals\n",
    "\n",
    "    # functional but not elegant (nor fast probably)\n",
    "    for k in range(nsamples):\n",
    "        id = df.iloc[k]['ID']\n",
    "        path_data = os.path.join(path + 'data/clips/', id+'.mp3')\n",
    "        waveform, rate = lb.load(path_data, sr=16*1e3)\n",
    "        df.at[k, 'audio_signal'] = waveform\n",
    "\n",
    "        if k % 100 == 0:\n",
    "            print('file '+ str(k))\n",
    "\n",
    "    # store as faster feather format\n",
    "    df[:nsamples].to_feather(path + 'data/ASR_train_audio'+str(nsamples)+'.ft')\n",
    "\n",
    "    #\n",
    "    df = df[:nsamples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a60b1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exist\n"
     ]
    }
   ],
   "source": [
    "## read test set into memory & store as .feather\n",
    "df_test = pd.read_csv(path + 'data/ASR_test.csv')\n",
    "nsamples = len(df_test)\n",
    "\n",
    "# check if already existent\n",
    "if os.path.isfile(path + 'data/ASR_test_audio'+str(nsamples)+'.ft'):\n",
    "    print (\"File exist\")\n",
    "    df_test = pd.read_feather(path + 'data/ASR_test_audio'+str(nsamples)+'.ft')\n",
    "else:\n",
    "    print(\"File does not exist\")\n",
    "\n",
    "    # initialize with list\n",
    "    audio_signals = len(df_test['ID'])*[[0]]\n",
    "    df_test['audio_signal'] = audio_signals\n",
    "\n",
    "    # functional but not elegant (nor fast probably)\n",
    "    for k in range(nsamples):\n",
    "        id = df_test.iloc[k]['ID']\n",
    "        path_data = os.path.join(path + 'data/clips/', id+'.mp3')\n",
    "        waveform, rate = lb.load(path_data, sr=16*1e3)\n",
    "        df_test.at[k, 'audio_signal'] = waveform\n",
    "\n",
    "        if k % 100 == 0:\n",
    "            print('file '+ str(k))\n",
    "\n",
    "    # store as faster feather format\n",
    "    df_test[:nsamples].to_feather(path + 'data/ASR_test_audio'+str(nsamples)+'.ft')\n",
    "\n",
    "    #\n",
    "    df_test = df_test[:nsamples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f5af6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "## train valid split\n",
    "# df_train -> used in optimization\n",
    "# df_valid -> used to evaluate model during optimization\n",
    "# df_valid2 -> independent set for testing\n",
    "df_train, df_valid2 = train_test_split(df, test_size=0.2, random_state=1234)\n",
    "df_train, df_valid = train_test_split(df_train, test_size=0.2, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07050423",
   "metadata": {},
   "outputs": [],
   "source": [
    "## dataset library (1-2GB/s data processing)\n",
    "from datasets import Dataset\n",
    "\n",
    "data_train = Dataset.from_pandas(df_train[['ID', 'transcription', 'audio_signal']])\n",
    "data_valid = Dataset.from_pandas(df_valid[['ID', 'transcription', 'audio_signal']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d0640b",
   "metadata": {},
   "source": [
    "### Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "621a5ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Lower casing (no punctuation included)\n",
    "import re\n",
    "chars_to_ignore= '[\\,\\?\\.\\!\\;\\:\\\"\\“\\%\\”\\�]'\n",
    "\n",
    "def remove_special_characters(batch):\n",
    "#     batch['text'] = batch[\"transcription\"].lower() + ' ' # lower casing + word separator at the end\n",
    "    batch['text'] = re.sub(chars_to_ignore, '', batch[\"transcription\"]).lower() + ' '\n",
    "    return batch\n",
    "\n",
    "data_train = data_train.map(remove_special_characters, remove_columns=['transcription'], num_proc=4)\n",
    "data_valid = data_valid.map(remove_special_characters, remove_columns=['transcription'], num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10801d87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1981fe77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef extract_all_chars(batch):\\n  all_text = \" \".join(batch[\\'text\\'])\\n  vocab = list(set(all_text))\\n  return {\"vocab\": [vocab], \"all_text\": [all_text]}\\n\\nvocab_train = data_train.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=data_train.column_names)\\nvocab_valid = data_valid.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=data_train.column_names)\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## build vocabulary\n",
    "# Q: exclude {, | -} ?\n",
    "'''\n",
    "def extract_all_chars(batch):\n",
    "    all_text = \" \".join(batch['text'])\n",
    "    vocab = list(set(all_text))\n",
    "    return {\"vocab\": [vocab], \"all_text\": [all_text]}\n",
    "\n",
    "vocab_train = data_train.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=data_train.column_names)\n",
    "vocab_valid = data_valid.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=data_train.column_names)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3b7f17b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n## creating vocab dictionary\\nimport json\\nimport os\\n\\n#\\nvocab_list = list(set(vocab_train[\"vocab\"][0]))\\n\\nvocab_dict = {v: k for k, v in enumerate(vocab_list)}\\nprint(vocab_dict)\\n\\n# add unknown token, blank token\\nvocab_dict[\"[UNK]\"] = len(vocab_dict)\\nvocab_dict[\"[PAD]\"] = len(vocab_dict)\\n\\nprint(len(vocab_dict)) # dim required for linear layer\\n\\nvocab_path = os.path.join(path, \"data/vocab.json\")\\nwith open(vocab_path, \\'w\\') as vocab_file:\\n    json.dump(vocab_dict, vocab_file)\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "## creating vocab dictionary\n",
    "import json\n",
    "import os\n",
    "\n",
    "#\n",
    "vocab_list = list(set(vocab_train[\"vocab\"][0]))\n",
    "\n",
    "vocab_dict = {v: k for k, v in enumerate(vocab_list)}\n",
    "print(vocab_dict)\n",
    "\n",
    "# add unknown token, blank token\n",
    "vocab_dict[\"[UNK]\"] = len(vocab_dict)\n",
    "vocab_dict[\"[PAD]\"] = len(vocab_dict)\n",
    "\n",
    "print(len(vocab_dict)) # dim required for linear layer\n",
    "\n",
    "vocab_path = os.path.join(path, \"data/vocab.json\")\n",
    "with open(vocab_path, 'w') as vocab_file:\n",
    "    json.dump(vocab_dict, vocab_file)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92ee6d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "## tokenizer (for output text)\n",
    "# tokenizer = Wav2Vec2CTCTokenizer(vocab_path, unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\" \")\n",
    "tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(\"facebook/wav2vec2-large-xlsr-53-french\") # IMPORTANT: before used Wav2VecTokenizer (not CTC)\n",
    "\n",
    "#tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "203157ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "## feature extractor (best guess: for input to cut into windows, normalize etc.)\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=True)\n",
    "\n",
    "## processor (combine tokenizer and feature extractor)\n",
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a1b7f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## extract input_values (normalization)\n",
    "def prepare_dataset(batch):\n",
    "    batch[\"input_values\"] = processor(batch[\"audio_signal\"], sampling_rate=16*1e3).input_values\n",
    "\n",
    "    with processor.as_target_processor():\n",
    "        batch[\"labels\"] = processor(batch[\"text\"]).input_ids\n",
    "    return batch\n",
    "\n",
    "# parameter num_proc does not exist in the currently used version of datasets\n",
    "data_train = data_train.map(prepare_dataset, remove_columns=data_train.column_names, batch_size=8,  batched=True, num_proc=4)\n",
    "data_valid = data_valid.map(prepare_dataset, remove_columns=data_valid.column_names, batch_size=8, batched=True, num_proc=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3feda7a1",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de82348e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wandb\n",
    "import wandb\n",
    "\n",
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "now = now.strftime(\"%d-%m-%Y_%H:%M\")\n",
    "\n",
    "wandb.init(project=\"ASR_Wolof\",\n",
    "           entity=\"andrschl\",\n",
    "           name = now,\n",
    "           group=\"Wav2Vec2.0_XLSR_large_french\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f4c429b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## data collator (dynamic padding)\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "        max_length (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
    "        max_length_labels (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    max_length_labels: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lenghts and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                max_length=self.max_length_labels,\n",
    "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        # input_values, attention_mask, labels\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "56d3c911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jiwer in /home/andrschl/.virtualenvs/zindi/lib/python3.8/site-packages (2.2.0)\n",
      "Requirement already satisfied: python-Levenshtein in /home/andrschl/.virtualenvs/zindi/lib/python3.8/site-packages (from jiwer) (0.12.2)\n",
      "Requirement already satisfied: numpy in /home/andrschl/.virtualenvs/zindi/lib/python3.8/site-packages (from jiwer) (1.20.2)\n",
      "Requirement already satisfied: setuptools in /home/andrschl/.virtualenvs/zindi/lib/python3.8/site-packages (from python-Levenshtein->jiwer) (44.0.0)\n"
     ]
    }
   ],
   "source": [
    "## metric\n",
    "!pip install jiwer\n",
    "from datasets import load_metric\n",
    "\n",
    "wer_metric = load_metric(\"wer\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    # argmax of softmax\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    # -100 id -> pad token\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    # prediction id -> character\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    # we do not want to group tokens when computing the metrics?\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c19f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## model\n",
    "## Note: play around with hyperparameters (take training to laptop and perform grid search?)\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    \"facebook/wav2vec2-large-xlsr-53-french\",\n",
    "    attention_dropout=0.1,\n",
    "    hidden_dropout=0.1,\n",
    "    feat_proj_dropout=0.0,\n",
    "    mask_time_prob=0.05,\n",
    "    layerdrop=0.1,\n",
    "    gradient_checkpointing=True, # save GPU memory\n",
    "    ctc_loss_reduction=\"mean\",\n",
    "    pad_token_id=processor.tokenizer.pad_token_id, # define pad token\n",
    "    #vocab_size=len(processor.tokenizer) -> mis-match of last layer due to vocab size\n",
    ")\n",
    "\n",
    "# TODO: remove output layer and replace with vocabulary sized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b87d158",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to('cuda');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8369a195",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Freeze CNN layers (no fine tuning as stated in paper)\n",
    "model.freeze_feature_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4791c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TESTING: freeze all layers\n",
    "\n",
    "# for name, param in model.named_parameters():\n",
    "#     # param.requires_grad = False\n",
    "#     if 'lm_head' not in name:\n",
    "#     param.requires_grad = False\n",
    "\n",
    "#     if param.requires_grad:\n",
    "#     print(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7585c990",
   "metadata": {},
   "outputs": [],
   "source": [
    "## hyperparameters\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "path_model = os.path.join(path, '/model/wav2vec2-large-xlsr-french_'+now)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=path + '/model/wav2vec2-large-xlsr-french_'+now,\n",
    "    group_by_length=True,\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=2,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    num_train_epochs=1,\n",
    "    fp16=True,\n",
    "    save_steps=100,\n",
    "    eval_steps=100,\n",
    "    logging_steps=70,\n",
    "    learning_rate=3e-4,\n",
    "    warmup_steps=20,\n",
    "    save_total_limit=1,\n",
    "    report_to=\"wandb\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5788ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Trainer\n",
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=data_train,\n",
    "    eval_dataset=data_valid,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d2fcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "## start training\n",
    "model.train()\n",
    "trainer.train()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b97798e",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "12cf63dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8111085071478791"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "#from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "if not 'model' in globals():\n",
    "    print('Load model')\n",
    "    model = Wav2Vec2ForCTC.from_pretrained(path + '/model/wav2vec2-large-xlsr-french-test/checkpoint-1650/').to(\"cuda\")\n",
    "\n",
    "model.eval();\n",
    "\n",
    "# new processor if not yet existent\n",
    "if not 'processor' in globals():\n",
    "    print('Load processor')\n",
    "    tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(\"facebook/wav2vec2-large-xlsr-53-french\")\n",
    "    feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=True)\n",
    "    processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
    "    # processor = Wav2Vec2Processor.from_pretrained('./drive/MyDrive/Colab Notebooks/model/wav2vec2-large-xlsr-french-test/checkpoint-150/') # for some reason the file is not found\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    return processor(batch, return_tensors=\"pt\", sampling_rate=16000)\n",
    "\n",
    "wer_ = []\n",
    "\n",
    "#\n",
    "input_dict = df_valid2['audio_signal'].apply(prepare_dataset)\n",
    "label_str = ''\n",
    "pred_str = ''\n",
    "\n",
    "for idx in range(len(df_valid2)):\n",
    "    #print('-----------------')\n",
    "    logits = model(input_dict.values[idx].input_values.to(\"cuda\")).logits\n",
    "    pred_ids = torch.argmax(logits, dim=-1)[0]\n",
    "\n",
    "    '''\n",
    "    pred = []\n",
    "    for word in processor.decode(pred_ids).split(' '):\n",
    "    sim = difflib.get_close_matches(word, xv, n=1)\n",
    "    if sim == []:\n",
    "      sim = word\n",
    "    else:\n",
    "      sim = sim[0]\n",
    "\n",
    "    pred.append(sim)\n",
    "\n",
    "    #print(\"Prediction:\")\n",
    "    pred_str+= ' '.join(pred)+ ' '\n",
    "    '''\n",
    "    pred_str+= processor.decode(pred_ids)+ ' '\n",
    "\n",
    "    #print(\"\\nReference:\")\n",
    "    label_str+= df_valid2[\"transcription\"].values[idx].lower()+ ' '\n",
    "\n",
    "    # need same length for wer_metric\n",
    "    #label_str = label_str.ljust(len(pred_str))\n",
    "    #pred_str = pred_str.ljust(len(label_str))\n",
    "wer_.append(wer_metric.compute(predictions=[pred_str], references=[label_str]))\n",
    "\n",
    "np.mean(wer_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "031d1cd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8111085071478791"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wer_metric.compute(predictions=[pred_str], references=[label_str])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "08402453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rue baffassene\n"
     ]
    }
   ],
   "source": [
    "audio_signal = df[df['ID']=='de242ae13e8bc5c14e01295dd52eb0d3cc5b7e224507a6decd664eecdad6de5d9fb81c592fae6aadf4830955c36e9620cf07ea38b07675e513b23c0842233596']['audio_signal'].values\n",
    "input_dict = processor(audio_signal[0], return_tensors=\"pt\", sampling_rate=16*1e3)\n",
    "\n",
    "logits = model(input_dict.input_values.to(\"cuda\")).logits\n",
    "pred_ids = torch.argmax(logits, dim=-1)[0]\n",
    "\n",
    "#print(\"Prediction:\")\n",
    "pred_str = processor.decode(pred_ids)\n",
    "print(pred_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed09a688",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0bc7b15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## prediction\n",
    "# load data (dataframe) -> empty entries\n",
    "df_test = pd.read_feather(path+'/data/ASR_test_audio1564.ft')\n",
    "df_test = df_test[['ID', 'audio_signal']]\n",
    "\n",
    "df_test.head()\n",
    "\n",
    "'''\n",
    "# pre-process (processor)\n",
    "from transformers import Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor, Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "\n",
    "tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(\"facebook/wav2vec2-large-xlsr-53-french\") #\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=True)\n",
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
    "\n",
    "# pass through model\n",
    "modelTest = Wav2Vec2ForCTC.from_pretrained('./drive/MyDrive/Colab Notebooks/model/wav2vec2-large-xlsr-french-test/checkpoint-280/').to(\"cuda\")\n",
    "'''\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    return processor(batch, return_tensors=\"pt\", sampling_rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "14d57986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run through processor\n",
    "import difflib\n",
    "input_dict = df_test['audio_signal'].apply(prepare_dataset)\n",
    "preds = []\n",
    "\n",
    "# run through model and decoder\n",
    "for i in range(len(df_test)):\n",
    "  logits = model(input_dict[i].input_values.to('cuda')).logits\n",
    "  pred_ids = torch.argmax(logits, dim=-1)\n",
    "  preds.append(processor.decode(pred_ids[0]))\n",
    "\n",
    "  '''\n",
    "  pred = []\n",
    "  for word in processor.decode(pred_ids[0]).split(' '):\n",
    "    sim = difflib.get_close_matches(word, xv, n=1)\n",
    "    if sim == []:\n",
    "      sim = word\n",
    "    else:\n",
    "      sim = sim[0]\n",
    "        \n",
    "    pred.append(sim)\n",
    "  \n",
    "  preds.append(''.join(pred))\n",
    "  '''\n",
    "\n",
    "\n",
    "# save as csv\n",
    "dfpred = pd.DataFrame(list(zip(list(df_test['ID'].values), preds)), columns=['ID', 'transcription'])\n",
    "dfpred['transcription'] = dfpred['transcription'].replace([''], ' ') # replace empty prediction with \" \"\n",
    "dfpred.to_csv(path+'/predictions/prediction_'+now+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6029e837",
   "metadata": {},
   "source": [
    "### Word model/ Nearest neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1cec2604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'25-04-2021_22:13'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "\n",
    "#\n",
    "words = df['transcription']\n",
    "allwords = []\n",
    "\n",
    "for wordlist in words:\n",
    "  allwords += list(wordlist.lower().split())\n",
    "\n",
    "# histogram\n",
    "mostcommon_small = FreqDist(allwords).most_common(10000)\n",
    "xv, yv = zip(*mostcommon_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b19ed6aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42934282848412836"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "\n",
    "#\n",
    "words = dfpred['transcription']\n",
    "allwords = []\n",
    "\n",
    "for wordlist in words:\n",
    "  allwords += list(wordlist.split())\n",
    "\n",
    "# histogram\n",
    "mostcommon_small = FreqDist(allwords).most_common(10000)\n",
    "xp, yp = zip(*mostcommon_small)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, figsize=(22,10))\n",
    "ax[0].bar(xv,yv)\n",
    "ax[1].bar(xp,yp)\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Frequency of Words')\n",
    "plt.xticks(rotation=60)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "22a09e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_all = 0\n",
    "count_notshared = 0\n",
    "count = 0\n",
    "for k, word in enumerate(xp):\n",
    "  count_all += yp[k]\n",
    "  if word not in xv:\n",
    "    count_notshared += yp[k]\n",
    "    #print(word+ ' '+ str(yp[k]))\n",
    "    count += 1\n",
    "\n",
    "print(count_all)\n",
    "print(count_notshared)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "cc7675a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a6bcc2b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "cc673122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                       affricatell avées\n",
       "1       nanlayydie fon guerdem taliboubes\n",
       "2                         afficatell abes\n",
       "3                       moskue de camerin\n",
       "4                 cité safkot tivoion pol\n",
       "                      ...                \n",
       "1559              bankooff affrica pikine\n",
       "1560                          tigo almadi\n",
       "1561                          wigi gramba\n",
       "1562              pharmacie rokayya wakam\n",
       "1563                       hora la banque\n",
       "Name: transcription, Length: 1564, dtype: object"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1a24e221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>transcription</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00416cff4f818d3dfd99c9178ff0e268e7575500c8baa5...</td>\n",
       "      <td>affricatell avées</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00891ba561e80e135f9d12b9fa1347f0a2560998f7ea16...</td>\n",
       "      <td>nanlayydie fon guerdem taliboubes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00a508027ed4edf0bd3db79f45f4ed6e1b89fba6482c10...</td>\n",
       "      <td>afficatell abes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00ac13cd0d93e35c1ff672cc106ad94d1ea9b93fcf049a...</td>\n",
       "      <td>moskue de camerin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00c2d5baf4719bf01b990a8924e99bda043cd462147193...</td>\n",
       "      <td>cité safkot tivoion pol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1559</th>\n",
       "      <td>ff0da457e7a3986035995912803e42261c5f5f448c126b...</td>\n",
       "      <td>bankooff affrica pikine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1560</th>\n",
       "      <td>ff1808218a15fa576c405314e4de4bda56c44f849ff1b5...</td>\n",
       "      <td>tigo almadi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1561</th>\n",
       "      <td>ff5b9a45d60600e875e0a031b1d7076c9cbdeb1c48c09c...</td>\n",
       "      <td>wigi gramba</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1562</th>\n",
       "      <td>ff98e108ec61d3bd485734b83f21be77820549dab1cac1...</td>\n",
       "      <td>pharmacie rokayya wakam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1563</th>\n",
       "      <td>ffb6873f183e8995e50d1079f60f8e9d1018092e421578...</td>\n",
       "      <td>hora la banque</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1564 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     ID  \\\n",
       "0     00416cff4f818d3dfd99c9178ff0e268e7575500c8baa5...   \n",
       "1     00891ba561e80e135f9d12b9fa1347f0a2560998f7ea16...   \n",
       "2     00a508027ed4edf0bd3db79f45f4ed6e1b89fba6482c10...   \n",
       "3     00ac13cd0d93e35c1ff672cc106ad94d1ea9b93fcf049a...   \n",
       "4     00c2d5baf4719bf01b990a8924e99bda043cd462147193...   \n",
       "...                                                 ...   \n",
       "1559  ff0da457e7a3986035995912803e42261c5f5f448c126b...   \n",
       "1560  ff1808218a15fa576c405314e4de4bda56c44f849ff1b5...   \n",
       "1561  ff5b9a45d60600e875e0a031b1d7076c9cbdeb1c48c09c...   \n",
       "1562  ff98e108ec61d3bd485734b83f21be77820549dab1cac1...   \n",
       "1563  ffb6873f183e8995e50d1079f60f8e9d1018092e421578...   \n",
       "\n",
       "                          transcription  \n",
       "0                     affricatell avées  \n",
       "1     nanlayydie fon guerdem taliboubes  \n",
       "2                       afficatell abes  \n",
       "3                     moskue de camerin  \n",
       "4               cité safkot tivoion pol  \n",
       "...                                 ...  \n",
       "1559            bankooff affrica pikine  \n",
       "1560                        tigo almadi  \n",
       "1561                        wigi gramba  \n",
       "1562            pharmacie rokayya wakam  \n",
       "1563                     hora la banque  \n",
       "\n",
       "[1564 rows x 2 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dc2d01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
