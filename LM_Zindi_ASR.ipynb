{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"LM_Zindi_ASR.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyONZOs/fFTxyWr9SkTH6NdD"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"6Tw0Lqm5073z"},"source":["### Setup"]},{"cell_type":"code","metadata":{"id":"_CktwOwj01mM"},"source":["## path\n","path = 'drive/MyDrive/Colab Notebooks/Zindi ASR/' # local path\n","\n","#\n","path_data = path+'data/ASR_Zindi/'\n","path_save = path+ 'predictionsLM_24MayXModelsLM.csv'\n","\n","# ensemble\n","model1_name = path+'models/Model1'\n","model2_name = path+'models/Model2'\n","model3_name = path+'models/Model3'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PpV0HxP4jCdC","executionInfo":{"elapsed":946,"status":"ok","timestamp":1621882876205,"user":{"displayName":"Roman Engeler","photoUrl":"","userId":"01825020783678319162"},"user_tz":-120},"outputId":"ed54b654-141f-4c4c-aa5f-92ff66c90455"},"source":["## mount drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AAO8Kt4_1WlF"},"source":["%%capture\n","!pip install datasets # to use\n","!pip install git+https://github.com/huggingface/transformers # to user huggingface transformer\n","!pip install jiwer # for wer metric\n","\n","!pip install -U pip\n","!pip install -U dill\n","!pip install -U nltk==3.4"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l7iUidpl1aZ6"},"source":["## load packages\n","# standard python\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import random\n","import os.path\n","import math\n","from operator import itemgetter\n","\n","# preprocessing\n","import librosa as lb\n","import re\n","from datasets import load_metric\n","from sklearn.model_selection import train_test_split\n","from scipy.stats import entropy\n","\n","# torch\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","# transformers\n","from transformers import Wav2Vec2ForCTC, Wav2Vec2CTCTokenizer\n","from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Processor\n","\n","# language model\n","from nltk.util import pad_sequence\n","from nltk.util import ngrams, bigrams\n","from nltk.lm.preprocessing import pad_both_ends, padded_everygram_pipeline\n","from nltk.lm.preprocessing import flatten\n","from nltk.lm import MLE, KneserNeyInterpolated\n","\n","# nearest neighbor\n","import difflib"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gNqhX5Jf1lFh"},"source":["## Seeding\n","random.seed(10)\n","np.random.seed(10)\n","torch.manual_seed(10)\n","torch.cuda.manual_seed_all(10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UVB9MG970-Zx"},"source":["### Data"]},{"cell_type":"code","metadata":{"id":"MILC_uop1nCg"},"source":["## Read data into memory (small)\n","df = pd.read_feather(path_data+'ASR_train_audio6683.ft')\n","\n","# train valid split\n","df_train, df_valid = train_test_split(df, test_size=0.2, random_state=1234)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QC7SJwHPPaTC"},"source":["## word corpus for nearest neighbor\n","from nltk.probability import FreqDist\n","from wordcloud import WordCloud, ImageColorGenerator\n","\n","## Zindi dataset\n","words = filter(None, [re.sub('[,().?!~;1234567890^]', '', word.lower()) for word in list(df['transcription'].values)])\n","allwords = []\n","\n","for wordlist in words:\n","  allwords += list(wordlist.lower().split())\n","\n","# histogram. time & space complexity linear in data set size\n","mostcommon_small = FreqDist(allwords).most_common(1500) # it has around 1000 distinct words -> 1500 to be sure that all are included\n","xv, yv = zip(*mostcommon_small)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GeCeYZuQjIkm"},"source":["### Re-evaluate Model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mQOfoGgp1ByH","executionInfo":{"elapsed":111829,"status":"ok","timestamp":1621882987121,"user":{"displayName":"Roman Engeler","photoUrl":"","userId":"01825020783678319162"},"user_tz":-120},"outputId":"ce061e48-34e4-4a56-fc34-32aa4b7da811"},"source":["## Re-evaluate performance of model\n","\n","# load XLSR model\n","if not 'XLSRmodel1' in globals():\n","  print('Load model')\n","  XLSRmodel1 = Wav2Vec2ForCTC.from_pretrained(model1_name).to(\"cuda\")\n","  XLSRmodel2 = Wav2Vec2ForCTC.from_pretrained(model2_name).to(\"cuda\")\n","  XLSRmodel3 = Wav2Vec2ForCTC.from_pretrained(model3_name).to(\"cuda\") \n","\n","# load processor\n","if not 'processor' in globals():\n","  print('Load processor')\n","  tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(\"facebook/wav2vec2-large-xlsr-53-french\")\n","  feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=True)\n","  processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n","\n","# prepare dataset\n","def prepare_dataset(batch):\n","    return processor(batch, return_tensors=\"pt\", sampling_rate=16*1e3)\n","input_dict = df_valid['audio_signal'].apply(prepare_dataset)\n","\n","# word error rate\n","wer_metric = load_metric(\"wer\")\n","wer_ = []\n","\n","# entropy\n","softmax = nn.Softmax(dim=2)\n","entropy_ = []\n","\n","# WER over everything (one long string)\n","label_str = ''\n","pred_str = ''\n","\n","for idx in range(len(df_valid)):\n","  #print('-----------------')\n","  logits = XLSRmodel3(input_dict.values[idx].input_values.to(\"cuda\")).logits\n","\n","  pred_ids = torch.argmax(logits, dim=-1)[0]\n","\n","  # WER over everything (one long string)\n","  pred_str+= processor.decode(pred_ids)+ ' '\n","  label_str+= df_valid[\"transcription\"].values[idx].lower()+ ' '\n","\n","  # entropy\n","  entropy_.append(np.mean(entropy(softmax(logits).detach().to(\"cpu\")[0].numpy())))\n","\n","wer_.append(wer_metric.compute(predictions=[pred_str], references=[label_str]))\n","\n","print(np.mean(wer_))\n","print(np.mean(entropy_))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Load model\n","Load processor\n","0.040309350831966254\n","1.6866072\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"OgboLRG43Hpp"},"source":["### Beam Search"]},{"cell_type":"code","metadata":{"id":"Oh7Hwn5g3AEx"},"source":["## Beam Search\n","def beam_search_decoder(predictions, top_k = 3):\n","    #start with an empty sequence with zero score\n","    output_sequences = [([], 0)]\n","    \n","    #looping through all the predictions\n","    for token_probs in predictions:\n","        new_sequences = []\n","        \n","        #append new tokens to old sequences and re-score\n","        for old_seq, old_score in output_sequences:\n","            for char_index in range(len(token_probs)):\n","                new_seq = old_seq + [char_index]\n","                #considering log-likelihood for scoring\n","                new_score = old_score + math.log(token_probs[char_index])\n","                new_sequences.append((new_seq, new_score))\n","                \n","        #sort all new sequences in the de-creasing order of their score\n","        output_sequences = sorted(new_sequences, key = lambda val: val[1], reverse = True)\n","        \n","        #select top-k based on score \n","        # *Note- best sequence is with the highest score\n","        output_sequences = output_sequences[:top_k]\n","        \n","    return output_sequences"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"80Mhxc6wpAW-","executionInfo":{"elapsed":112120,"status":"ok","timestamp":1621882987422,"user":{"displayName":"Roman Engeler","photoUrl":"","userId":"01825020783678319162"},"user_tz":-120},"outputId":"a78f32dc-cd6c-4620-c2c7-9044f2866ca4"},"source":["## Test beam search \n","idx = 10\n","nbeams = 10\n","softmax = nn.Softmax(dim=2)\n","\n","#\n","pred = []\n","input_dict = df_train['audio_signal'][idx:idx+1].apply(prepare_dataset)\n","\n","for idx in range(len(input_dict)):\n","  #print('-----------------')\n","  logits = XLSRmodel1(input_dict.values[idx].input_values.to(\"cuda\")).logits\n","  # sum_j(output_ij) = 1 where i is column and j is row\n","  output = softmax(logits) # logits -> probabilities\n","\n","  beams_int = beam_search_decoder(torch.squeeze(output).tolist(), top_k = nbeams) # beams\n","  beams_str = nbeams*['']\n","\n","  for k in range(nbeams):\n","    pred_ids, pred_prob = beams_int[k]\n","    print(processor.decode(pred_ids))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["marché yeumbeul laa bëgg dem\n","marché yeumbeul laa bëgg dem\n","marché yeumbeul laa bëgg dem\n","marché yeumbeul laa bëgg dem\n","marché yeumbeul laa bëgg dem\n","marché yeumbeul laa bëgg dem\n","marché yeumbeul laa bëgg dem\n","marché yeumbeul laa bëgg dem\n","marché yeumbeul laa bëgg dem\n","marché yeumbeul laa bëgg dem\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"I3F24ZyA3LQ9"},"source":["### Word Level Language Model"]},{"cell_type":"code","metadata":{"id":"uhXsAzIWQ6S-"},"source":["## Fitting of n-gram LM\n","def train_ngram(LMmodel, data):\n","  '''\n","  input: model, list of sentences\n","  output: trained model\n","  '''\n","  #\n","  sentence_list = [sentence for sentence in data]\n","\n","  # lower casing\n","  word_list_lower = [list(filter(None, [re.sub('[,().?!~;1234567890^]', '', word.lower()) for word in sentence.split(' ')])) for sentence in sentence_list]\n","\n","  # preprocess for language model\n","  train_data, padded_words = padded_everygram_pipeline(3, word_list_lower)\n","  \n","  # fit model\n","  LMmodel.fit(train_data, padded_words)\n","\n","  return LMmodel"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z4x8hRidS_nG"},"source":["## Get perplexity of sentence\n","def ngram_logprobability(sentence):\n","    log_prob = 0\n","    count = 0\n","    for ngram in sentence:\n","      # there is probably an error in the implementation of KneserNeyInterpolated -> revert to MLE\n","      '''try:\n","        log_prob += np.log(LMKmodel.score(ngram[2], [ngram[0], ngram[1]]))\n","      except:\n","      '''\n","      # to avoid log(0) for unknown chars => many methods exist in the literature such as smoothing\n","      log_prob += np.log(LMmodel.score(ngram[2], [ngram[0], ngram[1]])+ 1e-8)\n","      count += 1\n","    return np.power(np.exp(log_prob), 1/count) # (inverse) perplexity to account for different word/ sentence length"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"72-oKWPoTESq"},"source":["## Get perplexity of list of sentences\n","def prob_sentences(sentences):\n","  '''\n","  input: list of sentences\n","  output: log probability for sentences\n","  '''\n","  # list with log probabilities\n","  log_probs = len(sentences)* [-np.infty]\n","\n","  # creating list of sentences from string\n","  list_sentences = [sentence.split(' ') for sentence in sentences]\n","\n","  # lower casing\n","  for k in range(len(list_sentences)):\n","    list_sentences[k] = [word.lower() for word in list_sentences[k]]\n","\n","  # list(sentence_list(word_list(ngrams)))\n","  list_ngrams = [list(ngrams(pad_both_ends(sentence, n=3), n=3)) for sentence in list_sentences]\n","\n","  for k, sentence in enumerate(list_ngrams):\n","    log_probs[k] = ngram_logprobability(sentence)\n","\n","  return log_probs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BRCL1XtBPMXd"},"source":["## Language model\n","LMKmodel = KneserNeyInterpolated(3) # Lets train a n-gram model KneserNeyInterpolated(n)\n","LMKmodel = train_ngram(LMKmodel, df_train['transcription'].values)\n","\n","# since there seems to be an error in the implementation of KneyerNey leading to errors -> fallback to standard MLE\n","LMmodel = MLE(3) # Lets train a n-gram model KneserNeyInterpolated(n)\n","LMmodel = train_ngram(LMmodel, df_train['transcription'].values)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DzW0gVN_TIjq"},"source":["### Validation"]},{"cell_type":"code","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"kXy3g_7qrScm","outputId":"38855f0f-cb84-46ed-cbe1-7a8d180c4e39"},"source":["## Predictions\n","nbeams = 20 # this should scale roughly linearly to runtime -> with 10 runs for about 10 mins\n","alpha = 0.\n","\n","# pre-processing of data\n","input_dict = df_valid['audio_signal'].apply(prepare_dataset)\n","\n","# word error rate\n","wer_ = []\n","\n","#\n","softmax = nn.Softmax(dim=2)\n","\n","# strings to calculate wer\n","label_str = ''\n","pred_str = ''\n","\n","# loop through validation set\n","for idx in range(len(df_valid)):\n","  # Model 1 --------\n","  logits = XLSRmodel1(input_dict.values[idx].input_values.to(\"cuda\")).logits\n","  output = softmax(logits) # logits -> probabilities\n","\n","  # beam search\n","  beams_int = beam_search_decoder(torch.squeeze(output).tolist(), top_k = nbeams) # beams\n","  beams_str = 3*nbeams*['']\n","  beams_XLSR_prob = 3*nbeams*[0]\n","\n","  for k in range(nbeams):\n","    pred_ids, pred_prob = beams_int[k]\n","    beams_XLSR_prob[k] = np.exp(pred_prob)\n","    beams_str[k] = processor.decode(pred_ids)\n","\n","  ## Model 2 --------\n","  logits = XLSRmodel2(input_dict.values[idx].input_values.to(\"cuda\")).logits\n","  output = softmax(logits) # logits -> probabilities\n","\n","  # beam search\n","  beams_int = beam_search_decoder(torch.squeeze(output).tolist(), top_k = nbeams) # beams\n","\n","  for k in range(nbeams):\n","    pred_ids, pred_prob = beams_int[k]\n","    beams_XLSR_prob[nbeams+ k] = np.exp(pred_prob)\n","    beams_str[nbeams+ k] = processor.decode(pred_ids)\n","\n","  ## Model 3 --------\n","  logits = XLSRmodel3(input_dict.values[idx].input_values.to(\"cuda\")).logits\n","  output = softmax(logits) # logits -> probabilities\n","\n","  # beam search\n","  beams_int = beam_search_decoder(torch.squeeze(output).tolist(), top_k = nbeams) # beams\n","\n","  for k in range(nbeams):\n","    pred_ids, pred_prob = beams_int[k]\n","    beams_XLSR_prob[2*nbeams+ k] = np.exp(pred_prob)\n","    beams_str[2*nbeams+ k] = processor.decode(pred_ids)\n","\n","  if idx% 100 == 0:\n","    print(idx)\n","\n","  # max prediction\n","  beams_LM_prob = prob_sentences(beams_str)\n","  pred_ = beams_str[np.argmax([pLM+alpha*pXLSR for pLM, pXLSR in zip(beams_LM_prob, beams_XLSR_prob)])].replace('<unk>', '')\n","\n","  # nearest neighbor\n","  pred = []\n","  for word in pred_.split(' '):\n","    sim = difflib.get_close_matches(word, xv, n=1)\n","\n","    if sim == []:\n","      sim = word\n","    else:\n","      sim = sim[0]\n","\n","    pred.append(sim)\n","  \n","  # append to prediction\n","  pred_str += ' '.join(pred)+ ' '\n","  label_str+= df_valid[\"transcription\"].values[idx].lower()+ ' '\n","  \n","# calculate wer\n","wer_.append(wer_metric.compute(predictions=[pred_str], references=[label_str]))\n","print(np.mean(wer_))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0\n","100\n","200\n","300\n","400\n","500\n","600\n","700\n","800\n","900\n","1000\n","1100\n","1200\n","1300\n","0.01687368174361378\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FtWwOM_31G1E"},"source":["### Prediction"]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"4cAyWHup1IGk"},"source":["## prediction\n","# load data (dataframe) -> empty entries\n","df_test = pd.read_feather(path_data+ 'ASR_test_audio1564.ft')\n","df_test = df_test[['ID', 'audio_signal']]\n","df_test.head()\n","\n","def prepare_dataset(batch):\n","    return processor(batch, return_tensors=\"pt\", sampling_rate=16*1e3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h4tNbrEyNLnJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621885626698,"user_tz":-120,"elapsed":12236,"user":{"displayName":"Roman Engeler","photoUrl":"","userId":"01825020783678319162"}},"outputId":"b24d9cce-4775-4fe8-a6c2-134a7d6cb08c"},"source":["# run through pre-processor\n","input_dict = df_test['audio_signal'].apply(prepare_dataset)\n","\n","# to store predictions\n","preds = []\n","\n","# beams\n","nbeams = 20\n","alpha = 0.\n","\n","#\n","softmax = nn.Softmax(dim=2)\n","\n","# run through model and decoder\n","for idx in range(len(df_test)):\n","  ## Model 1\n","  logits = XLSRmodel1(input_dict.values[idx].input_values.to(\"cuda\")).logits\n","  # sum_j(output_ij) = 1 where i is column and j is row\n","  output = softmax(logits) # logits -> probabilities\n","\n","  # beam search\n","  beams_int = beam_search_decoder(torch.squeeze(output).tolist(), top_k = nbeams) # beams\n","  beams_str = 3*nbeams*['']\n","  beams_XLSR_prob = 3*nbeams*[0]\n","\n","  for k in range(nbeams):\n","    pred_ids, pred_prob = beams_int[k]\n","    beams_XLSR_prob[k] = np.exp(pred_prob)\n","    beams_str[k] = processor.decode(pred_ids)\n","\n","  ## Model 2\n","  logits = XLSRmodel2(input_dict.values[idx].input_values.to(\"cuda\")).logits\n","  # sum_j(output_ij) = 1 where i is column and j is row\n","  output = softmax(logits) # logits -> probabilities\n","\n","  # beam search\n","  beams_int = beam_search_decoder(torch.squeeze(output).tolist(), top_k = nbeams) # beams\n","\n","  for k in range(nbeams):\n","    pred_ids, pred_prob = beams_int[k]\n","    beams_XLSR_prob[nbeams+ k] = np.exp(pred_prob)\n","    beams_str[nbeams+ k] = processor.decode(pred_ids)\n","\n","  ## Model 3\n","  logits = XLSRmodel3(input_dict.values[idx].input_values.to(\"cuda\")).logits\n","  # sum_j(output_ij) = 1 where i is column and j is row\n","  output = softmax(logits) # logits -> probabilities\n","\n","  # beam search\n","  beams_int = beam_search_decoder(torch.squeeze(output).tolist(), top_k = nbeams) # beams\n","\n","  for k in range(nbeams):\n","    pred_ids, pred_prob = beams_int[k]\n","    beams_XLSR_prob[2*nbeams+ k] = np.exp(pred_prob)\n","    beams_str[2*nbeams+ k] = processor.decode(pred_ids)\n","\n","  if idx% 100 == 0:\n","    print(idx)\n","\n","  # append prediction\n","  #pred_str+= ' '.join(pred).replace('<unk>', '')+ ' '\n","  beams_LM_prob = prob_sentences(beams_str)\n","  pred_str = beams_str[np.argmax([pLM+alpha*pXLSR for pLM, pXLSR in zip(beams_LM_prob, beams_XLSR_prob)])].replace('<unk>', '')\n","\n","  # nearest neighbor\n","  pred = []\n","  for word in pred_str.split(' '):\n","    sim = difflib.get_close_matches(word, xv, n=1)\n","\n","    if sim == []:\n","      sim = word\n","    else:\n","      sim = sim[0]\n","\n","    pred.append(sim)\n","  \n","  # append to prediction\n","  preds.append(' '.join(pred).replace('\"', ''))\n","  #preds.append(pred_str)\n","\n","# save as csv\n","dfpred = pd.DataFrame(list(zip(list(df_test['ID'].values), preds)), columns=['ID', 'transcription'])\n","dfpred.to_csv(path_save, index=False)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["0\n","100\n","200\n","300\n","400\n","500\n","600\n","700\n","800\n","900\n","1000\n","1100\n","1200\n","1300\n","1400\n","1500\n"],"name":"stdout"}]}]}