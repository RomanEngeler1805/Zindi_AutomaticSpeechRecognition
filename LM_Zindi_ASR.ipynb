{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LM_Zindi_ASR.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyMbxXugSLVYZfzXTFIx7/+/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"6Tw0Lqm5073z"},"source":["### Setup"]},{"cell_type":"code","metadata":{"id":"_CktwOwj01mM","executionInfo":{"status":"ok","timestamp":1617574728832,"user_tz":-120,"elapsed":662,"user":{"displayName":"Roman Engeler","photoUrl":"","userId":"01825020783678319162"}}},"source":["## path\n","path = 'drive/MyDrive/Colab Notebooks/'"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"AAO8Kt4_1WlF","executionInfo":{"status":"ok","timestamp":1617576619800,"user_tz":-120,"elapsed":28238,"user":{"displayName":"Roman Engeler","photoUrl":"","userId":"01825020783678319162"}}},"source":["%%capture\n","!pip install datasets # to use\n","!pip install git+https://github.com/huggingface/transformers # to user huggingface transformer\n","!pip install jiwer # for wer metric\n","\n","!pip install -U pip\n","!pip install -U dill\n","!pip install -U nltk==3.4"],"execution_count":42,"outputs":[]},{"cell_type":"code","metadata":{"id":"l7iUidpl1aZ6","executionInfo":{"status":"ok","timestamp":1617574758724,"user_tz":-120,"elapsed":30539,"user":{"displayName":"Roman Engeler","photoUrl":"","userId":"01825020783678319162"}}},"source":["## load packages\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import random\n","import os.path\n","\n","import torch\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from transformers import Wav2Vec2ForCTC, Wav2Vec2CTCTokenizer\n","from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Processor\n","\n","from datasets import load_metric\n","\n","import librosa as lb\n","\n","from sklearn.model_selection import train_test_split\n","\n","from nltk.util import pad_sequence\n","from nltk.util import ngrams, bigrams\n","from nltk.lm.preprocessing import pad_both_ends\n","from nltk.lm.preprocessing import flatten"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"gNqhX5Jf1lFh","executionInfo":{"status":"ok","timestamp":1617574758727,"user_tz":-120,"elapsed":30536,"user":{"displayName":"Roman Engeler","photoUrl":"","userId":"01825020783678319162"}}},"source":["# seeding\n","random.seed(10)\n","np.random.seed(10)\n","torch.manual_seed(10)\n","torch.cuda.manual_seed_all(10)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DoQppKuT1nst","executionInfo":{"status":"ok","timestamp":1617574758728,"user_tz":-120,"elapsed":30532,"user":{"displayName":"Roman Engeler","photoUrl":"","userId":"01825020783678319162"}},"outputId":"ee3f9675-0f33-450a-c148-08507c72b632"},"source":["## mount drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UVB9MG970-Zx"},"source":["### Data"]},{"cell_type":"code","metadata":{"id":"MILC_uop1nCg","executionInfo":{"status":"ok","timestamp":1617574794153,"user_tz":-120,"elapsed":65949,"user":{"displayName":"Roman Engeler","photoUrl":"","userId":"01825020783678319162"}}},"source":["## read into memory (small)\n","nsamples = 5760 # options {}\n","df = pd.read_feather('drive/MyDrive/Colab Notebooks/data/ASR_train_audio6683.ft')"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"vAoYH-gv1AA8","executionInfo":{"status":"ok","timestamp":1617574794156,"user_tz":-120,"elapsed":65947,"user":{"displayName":"Roman Engeler","photoUrl":"","userId":"01825020783678319162"}}},"source":["## train valid split\n","df_train, df_valid = train_test_split(df, test_size=0.2)"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M9ev0iGy1AyJ"},"source":["### XLSR Model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mQOfoGgp1ByH","executionInfo":{"status":"ok","timestamp":1617574921908,"user_tz":-120,"elapsed":193694,"user":{"displayName":"Roman Engeler","photoUrl":"","userId":"01825020783678319162"}},"outputId":"bf5ae188-738f-4123-8707-1c66b54f4cbc"},"source":["##\n","# load XLSR model\n","if not 'XLSRmodel' in globals():\n","  print('Load model')\n","  XLSRmodel = Wav2Vec2ForCTC.from_pretrained('./drive/MyDrive/Colab Notebooks/model/wav2vec2-large-xlsr-french-test/checkpoint-750/').to(\"cuda\")\n","\n","# load processor\n","if not 'processor' in globals():\n","  print('Load processor')\n","  tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(\"facebook/wav2vec2-large-xlsr-53-french\")\n","  feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=True)\n","  processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n","\n","# prepare dataset\n","def prepare_dataset(batch):\n","    return processor(batch, return_tensors=\"pt\", sampling_rate=16*1e3)\n","\n","# word error rate\n","wer_metric = load_metric(\"wer\")\n","wer_ = []\n","\n","#\n","input_dict = df_valid['audio_signal'].apply(prepare_dataset)\n","\n","for idx in range(len(df_valid)):\n","  #print('-----------------')\n","  logits = XLSRmodel(input_dict.values[idx].input_values.to(\"cuda\")).logits\n","  pred_ids = torch.argmax(logits, dim=-1)[0]\n","\n","  #print(\"Prediction:\")\n","  pred_str = processor.decode(pred_ids)\n","  #print(pred_str)\n","\n","  #print(\"\\nReference:\")\n","  label_str = df_valid[\"transcription\"].values[idx].lower()\n","  #print(label_str)\n","\n","  # need same length for wer_metric\n","  label_str = label_str.ljust(len(pred_str))\n","  pred_str = pred_str.ljust(len(label_str))\n","  wer_.append(wer_metric.compute(predictions=[pred_str], references=[label_str]))\n","\n","print(np.mean(wer_))"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Load model\n","Load processor\n","0.10110470016977122\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Z352pFrf1CSN"},"source":["### Language Model"]},{"cell_type":"code","metadata":{"id":"D7l35GDh3AHc","executionInfo":{"status":"ok","timestamp":1617574921908,"user_tz":-120,"elapsed":193691,"user":{"displayName":"Roman Engeler","photoUrl":"","userId":"01825020783678319162"}}},"source":["# to be considered\n","# model produces output of the form (CTC)\n","# <pad> <pad> <pad> <pad> <pad> <pad> r <pad> <pad> o u u <pad> <pad> t <pad> <pad> e <pad>\n","# => do we perform beam search on this sequence or first clean up the <pad> tokens?"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OgboLRG43Hpp"},"source":["#### Beam Search"]},{"cell_type":"code","metadata":{"id":"Oh7Hwn5g3AEx","executionInfo":{"status":"ok","timestamp":1617574921909,"user_tz":-120,"elapsed":193686,"user":{"displayName":"Roman Engeler","photoUrl":"","userId":"01825020783678319162"}}},"source":["# Beam Search\n","# https://towardsdatascience.com/boosting-your-sequence-generation-performance-with-beam-search-language-model-decoding-74ee64de435a\n","\n","import math\n","\n","def beam_search_decoder(predictions, top_k = 3):\n","    #start with an empty sequence with zero score\n","    output_sequences = [([], 0)]\n","    \n","    #looping through all the predictions\n","    for token_probs in predictions:\n","        new_sequences = []\n","        \n","        #append new tokens to old sequences and re-score\n","        for old_seq, old_score in output_sequences:\n","            for char_index in range(len(token_probs)):\n","                new_seq = old_seq + [char_index]\n","                #considering log-likelihood for scoring\n","                new_score = old_score + math.log(token_probs[char_index])\n","                new_sequences.append((new_seq, new_score))\n","                \n","        #sort all new sequences in the de-creasing order of their score\n","        output_sequences = sorted(new_sequences, key = lambda val: val[1], reverse = True)\n","        \n","        #select top-k based on score \n","        # *Note- best sequence is with the highest score\n","        output_sequences = output_sequences[:top_k]\n","        \n","    return output_sequences"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I3F24ZyA3LQ9"},"source":["#### Language Model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"id":"NZ2OOznq3AC7","executionInfo":{"status":"ok","timestamp":1617574921909,"user_tz":-120,"elapsed":193682,"user":{"displayName":"Roman Engeler","photoUrl":"","userId":"01825020783678319162"}},"outputId":"35aa6de5-94c0-49c2-ee80-a284a70270ce"},"source":["# need to find optimal n of n-gram\n","\n","## Language Model (n-gram vs KenLM)\n","# https://surfertas.github.io/deeplearning/pytorch/2017/08/20/n-gram.html # pytorch code (NN parametrization of LM)\n","# https://web.stanford.edu/~jurafsky/slp3/slides/LM_4.pdf\n","# https://www.kaggle.com/alvations/n-gram-language-model-with-nltk # code taken from here\n","# https://web.stanford.edu/~jurafsky/slp3/old_oct19/3.pdf -> improvements to LM's\n","\n","df_lm = df_train[:5]\n","\n","from nltk.util import ngrams, bigrams\n","from nltk.lm.preprocessing import padded_everygram_pipeline\n","\n","from nltk.lm import MLE\n","\n","# padding\n","from nltk.lm.preprocessing import pad_both_ends\n","#print(list(pad_both_ends(df_lm['transcription'].values[0], n=2)))\n","#print(list(bigrams(pad_both_ends(df_lm['transcription'].values[0], n=2))))\n","\n","'''\n","# materialize\n","for ngramlize_sent in train_data:\n","    print(list(ngramlize_sent))\n","    print()\n","print('#############')\n","list(padded_sents)\n","'''"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"\\n# materialize\\nfor ngramlize_sent in train_data:\\n    print(list(ngramlize_sent))\\n    print()\\nprint('#############')\\nlist(padded_sents)\\n\""]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"cD79x0G12__v","executionInfo":{"status":"ok","timestamp":1617574921910,"user_tz":-120,"elapsed":193675,"user":{"displayName":"Roman Engeler","photoUrl":"","userId":"01825020783678319162"}}},"source":["def train_ngram(LMmodel, data):\n","  '''\n","  input: model, list of sentences\n","  output: trained model\n","  '''\n","\n","  # one long string of words\n","  word_list = ' '.join(data)\n","\n","  # one long list of words\n","  test = word_list.split(' ')\n","\n","  # lower casing\n","  test_lower = [list(map(str.lower, [sent]))[0]\n","                for sent in test]\n","\n","  # preprocess for language model\n","  train_data, padded_sents = padded_everygram_pipeline(2, test_lower)\n","  \n","  # fit model\n","  LMmodel.fit(train_data, padded_sents)\n","\n","  return LMmodel"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":86},"id":"YI3YS3j72_9O","executionInfo":{"status":"ok","timestamp":1617575538221,"user_tz":-120,"elapsed":1948,"user":{"displayName":"Roman Engeler","photoUrl":"","userId":"01825020783678319162"}},"outputId":"b0ade753-fc4b-404e-f034-53b7a861e6b5"},"source":["## language model\n","# IMPORTANT there seems to be a missmatch in the vocabulary (44 vs 49 chars)\n","LMmodel = MLE(2) # Lets train a 2-gram model\n","LMmodel = train_ngram(LMmodel, df_train['transcription'].values)\n","print(LMmodel.vocab)\n","print(len(tokenizer.get_vocab()))\n","\n","'''\n","print(LMmodel.counts['c'])\n","print(LMmodel.counts[['c']]['o'])  # P('o'|'c')\n","print(LMmodel.score('o', ['c']))\n","print(LMmodel.vocab.lookup([char for char in test_lower[5]]))\n","'''"],"execution_count":29,"outputs":[{"output_type":"stream","text":["<Vocabulary with cutoff=1 unk_label='<UNK>' and 44 items>\n","49\n"],"name":"stdout"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"\\nprint(LMmodel.counts['c'])\\nprint(LMmodel.counts[['c']]['o'])  # P('o'|'c')\\nprint(LMmodel.score('o', ['c']))\\nprint(LMmodel.vocab.lookup([char for char in test_lower[5]]))\\n\""]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"markdown","metadata":{"id":"_qvwRJ8l3Xiy"},"source":["#### Pipeline"]},{"cell_type":"code","metadata":{"id":"GJIQfTGZ2_6w","executionInfo":{"status":"ok","timestamp":1617575572877,"user_tz":-120,"elapsed":587,"user":{"displayName":"Roman Engeler","photoUrl":"","userId":"01825020783678319162"}}},"source":["def ngram_logprobability(sentence):\n","    log_prob = 0\n","    for words in sentence:\n","      for ngram in words:\n","        log_prob += np.log(LMmodel.score(ngram[1], [ngram[0]]))\n","    return log_prob"],"execution_count":30,"outputs":[]},{"cell_type":"code","metadata":{"id":"R_tgrLoB2_31","executionInfo":{"status":"ok","timestamp":1617575573363,"user_tz":-120,"elapsed":644,"user":{"displayName":"Roman Engeler","photoUrl":"","userId":"01825020783678319162"}}},"source":["def logprob_sentences(sentences):\n","  '''\n","  input: list of sentences\n","  output: log probability for sentences\n","  '''\n","  # list with log probabilities\n","  log_probs = len(sentences)* [-np.infty]\n","\n","  #\n","  list_sentences = [sentence.split(' ') for sentence in sentences]\n","\n","  # lower casing\n","  for k in range(len(list_sentences)):\n","    list_sentences[k] = [list(map(str.lower, [sent]))[0]\n","                        for sent in list_sentences[k]]\n","\n","  # list(sentence_list(word_list(ngrams)))\n","  list_ngrams = [[list(bigrams(pad_both_ends(word, n=2))) for word in sentence] for sentence in list_sentences]\n","\n","  for k, sentence in enumerate(list_ngrams):\n","    log_probs[k] = ngram_logprobability(sentence)\n","\n","  return log_probs"],"execution_count":31,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CTHSNpfq88_H","executionInfo":{"status":"ok","timestamp":1617575586012,"user_tz":-120,"elapsed":1024,"user":{"displayName":"Roman Engeler","photoUrl":"","userId":"01825020783678319162"}},"outputId":"12b14545-eadb-4430-976b-ed54c0ce48ef"},"source":["df_lm['transcription'].values"],"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['Sonatel des HLM', 'Thiossane', 'Ecole Castor Rufisque',\n","       'Terrain Basket', 'Pharmacie Baye Niasse'], dtype=object)"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o2CjLTGP2_vm","executionInfo":{"status":"ok","timestamp":1617575586447,"user_tz":-120,"elapsed":1023,"user":{"displayName":"Roman Engeler","photoUrl":"","userId":"01825020783678319162"}},"outputId":"5ee81a34-6ae7-4aba-adaf-c8085e801a9e"},"source":["logprob_sentences(df_lm['transcription'].values)"],"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[-39.11360219997962,\n"," -22.730951645635475,\n"," -49.00474037170049,\n"," -38.438208357909495,\n"," -43.390913710755314]"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"code","metadata":{"id":"6yQsqSimClN_"},"source":["#\n","input_dict = df_valid['audio_signal'].apply(prepare_dataset)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"81zQn2aG1EVU","executionInfo":{"status":"ok","timestamp":1617576500893,"user_tz":-120,"elapsed":104113,"user":{"displayName":"Roman Engeler","photoUrl":"","userId":"01825020783678319162"}},"outputId":"a2d04d86-87fc-4b2d-de12-b68f03442db7"},"source":["# ATTENTION: XLSR model seems to be overconfident -> places all probability mass on one logit\n","## Prections\n","nbeams = 5\n","\n","# word error rate\n","wer_ = []\n","\n","#\n","softmax = nn.Softmax(dim=2)\n","\n","for idx in range(len(df_valid)):\n","  #print('-----------------')\n","  logits = XLSRmodel(input_dict.values[idx].input_values.to(\"cuda\")).logits\n","  output = softmax(logits) # logits -> probabilities\n","\n","  # beam search\n","  beams_int = beam_search_decoder(torch.squeeze(output).tolist(), top_k = nbeams) # beams\n","  beams_str = nbeams*['']\n","\n","  for k in range(nbeams):\n","    pred_ids, pred_prob = beams_int[k]\n","    beams_str[k] = processor.decode(pred_ids)\n","\n","  # prediction P(final) = Alpha * P(model) + Beta * P(L.M.)\n","  logprob = logprob_sentences(beams_str)\n","  pred_str = beams_str[np.argmax(logprob)]\n","\n","  #print(\"\\nReference:\")\n","  label_str = df_valid[\"transcription\"].values[idx].lower()\n","\n","  # need same length for wer_metric\n","  label_str = label_str.ljust(len(pred_str))\n","  pred_str = pred_str.ljust(len(label_str))\n","\n","  wer_.append(wer_metric.compute(predictions=[pred_str], references=[label_str]))\n","\n","print(np.mean(wer_))"],"execution_count":41,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: RuntimeWarning: divide by zero encountered in log\n","  \"\"\"\n"],"name":"stderr"},{"output_type":"stream","text":["0.11455699208130025\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FtWwOM_31G1E"},"source":["### Prediction"]},{"cell_type":"code","metadata":{"id":"4cAyWHup1IGk","executionInfo":{"status":"aborted","timestamp":1617574922331,"user_tz":-120,"elapsed":194058,"user":{"displayName":"Roman Engeler","photoUrl":"","userId":"01825020783678319162"}}},"source":[""],"execution_count":null,"outputs":[]}]}