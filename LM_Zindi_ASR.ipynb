{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LM_Zindi_ASR.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMKlyL1fATcVTrDqXpFMuKX"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"6Tw0Lqm5073z"},"source":["### Setup"]},{"cell_type":"code","metadata":{"id":"_CktwOwj01mM","executionInfo":{"status":"ok","timestamp":1619034317970,"user_tz":-120,"elapsed":974,"user":{"displayName":"Roman Engeler","photoUrl":"","userId":"01825020783678319162"}}},"source":["## path\n","path = 'drive/MyDrive/Colab Notebooks/'"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"AAO8Kt4_1WlF","executionInfo":{"status":"ok","timestamp":1619034361815,"user_tz":-120,"elapsed":44811,"user":{"displayName":"Roman Engeler","photoUrl":"","userId":"01825020783678319162"}}},"source":["%%capture\n","!pip install datasets # to use\n","!pip install git+https://github.com/huggingface/transformers # to user huggingface transformer\n","!pip install jiwer # for wer metric\n","\n","!pip install -U pip\n","!pip install -U dill\n","!pip install -U nltk==3.4"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"l7iUidpl1aZ6","executionInfo":{"status":"ok","timestamp":1619034368348,"user_tz":-120,"elapsed":51337,"user":{"displayName":"Roman Engeler","photoUrl":"","userId":"01825020783678319162"}}},"source":["## load packages\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import random\n","import os.path\n","\n","import torch\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from transformers import Wav2Vec2ForCTC, Wav2Vec2CTCTokenizer\n","from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Processor\n","\n","from datasets import load_metric\n","\n","import librosa as lb\n","\n","from sklearn.model_selection import train_test_split\n","\n","from nltk.util import pad_sequence\n","from nltk.util import ngrams, bigrams\n","from nltk.lm.preprocessing import pad_both_ends\n","from nltk.lm.preprocessing import flatten"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"gNqhX5Jf1lFh","executionInfo":{"status":"ok","timestamp":1619034368349,"user_tz":-120,"elapsed":51331,"user":{"displayName":"Roman Engeler","photoUrl":"","userId":"01825020783678319162"}}},"source":["# seeding\n","random.seed(10)\n","np.random.seed(10)\n","torch.manual_seed(10)\n","torch.cuda.manual_seed_all(10)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DoQppKuT1nst","executionInfo":{"status":"ok","timestamp":1619034388539,"user_tz":-120,"elapsed":71514,"user":{"displayName":"Roman Engeler","photoUrl":"","userId":"01825020783678319162"}},"outputId":"f9a141a5-1104-4a55-d82c-bd94160d1a66"},"source":["## mount drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UVB9MG970-Zx"},"source":["### Data"]},{"cell_type":"code","metadata":{"id":"MILC_uop1nCg","executionInfo":{"status":"ok","timestamp":1619034400479,"user_tz":-120,"elapsed":83446,"user":{"displayName":"Roman Engeler","photoUrl":"","userId":"01825020783678319162"}}},"source":["## read into memory (small)\n","df = pd.read_feather('drive/MyDrive/Colab Notebooks/data/ASR_train_audio6683.ft')"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"vAoYH-gv1AA8","executionInfo":{"status":"ok","timestamp":1619034400480,"user_tz":-120,"elapsed":83440,"user":{"displayName":"Roman Engeler","photoUrl":"","userId":"01825020783678319162"}}},"source":["## train valid split\n","df_train, df_valid = train_test_split(df, test_size=0.2, random_state=1234)"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M9ev0iGy1AyJ"},"source":["### XLSR Model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"mQOfoGgp1ByH","executionInfo":{"status":"error","timestamp":1619034775517,"user_tz":-120,"elapsed":14242,"user":{"displayName":"Roman Engeler","photoUrl":"","userId":"01825020783678319162"}},"outputId":"c7da2438-222f-444e-ebba-423fe9592171"},"source":["##\n","# load XLSR model\n","if not 'XLSRmodel' in globals():\n","  print('Load model')\n","  XLSRmodel = Wav2Vec2ForCTC.from_pretrained('./drive/MyDrive/Colab Notebooks/model/wav2vec2-large-xlsr-french-11Apr/checkpoint-1200/').to(\"cuda\")\n","\n","# load processor\n","if not 'processor' in globals():\n","  print('Load processor')\n","  tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(\"facebook/wav2vec2-large-xlsr-53-french\")\n","  feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=True)\n","  processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n","\n","# prepare dataset\n","def prepare_dataset(batch):\n","    return processor(batch, return_tensors=\"pt\", sampling_rate=16*1e3)\n","\n","# word error rate\n","wer_metric = load_metric(\"wer\")\n","wer_ = []\n","\n","#\n","input_dict = df_valid['audio_signal'].apply(prepare_dataset)\n","\n","## WER over everything (one long string)\n","label_str = ''\n","pred_str = ''\n","\n","for idx in range(len(df_valid)):\n","  #print('-----------------')\n","  logits = XLSRmodel(input_dict.values[idx].input_values.to(\"cuda\")).logits\n","  pred_ids = torch.argmax(logits, dim=-1)[0]\n","\n","  ## WER per sentence and then average\n","  #pred_str = processor.decode(pred_ids)\n","  #label_str = df_valid[\"transcription\"].values[idx].lower()\n","\n","  ## WER over everything (one long string)\n","  pred_str+= processor.decode(pred_ids)+ ' '\n","  label_str+= df_valid[\"transcription\"].values[idx].lower()+ ' '\n","\n","  # need same length for wer_metric ? really\n","  #label_str = label_str.ljust(len(pred_str))\n","  #pred_str = pred_str.ljust(len(label_str))\n","\n","wer_.append(wer_metric.compute(predictions=[pred_str], references=[label_str]))\n","\n","print(np.mean(wer_))"],"execution_count":14,"outputs":[{"output_type":"stream","text":["----------------\n","ban car mooy dem cheikh ahmadou ban mackétaboune<unk> dakar\n","ban car mooy dem cheikh ahmadou bando mackétaboune<unk> dakar\n","----------------\n","rue séne\n","rue bafaséne\n","----------------\n","croisement taly diallo\n","croisement telry diallo\n","----------------\n","place de l’indépendance\n","place de l<unk>indépendance\n","----------------\n","agence axa assurance immo ngor\n","agen axta asurance immo ngor\n","----------------\n","ecole sherif youssou thiaw laye malika\n","ecole sherif oussénou thiaw laye malika\n","----------------\n","croisement camberéne\n","croisement camberénee\n","----------------\n","mbédou\n","mbédoufass\n","----------------\n","depot la layousse faouzy\n","depot la yousse faouzy\n","----------------\n","ndeureuhlou\n","deureuhlou\n","----------------\n","ban bus mooy jaar pont danguou\n","ban bus mooy jaar pont ndanguo\n","----------------\n","rond point les grands moulins de dakar\n","rond point les grands mouzains de dakar\n","----------------\n","lat - urbam\n","kat - urbam\n","----------------\n","rond point terrain dialoré\n","rond point terrain diadoré\n","----------------\n","ministère de la santé de la prévention\n","ministère de la santée de la prévention\n","----------------\n","taly bou khonkh station clean oil<unk>\n","taly bou khonkh <unk>station clen oil<unk>\n","----------------\n","chambre de pamecas\n","chambre de pamers\n","----------------\n","lycée camp marchande\n","lycée camp marchand\n","----------------\n","ministere de l'interieur senegal\n","ministere de l'interieur sengalé\n","----------------\n","stade des baol diop\n","stade des bal diop\n","----------------\n","seydina limamoulaye avenue dakar\n","seydina limamoulaye avenue<unk> dakar\n","----------------\n","pikine saf mar\n","pikine saf mbar\n","----------------\n","jardin botanique\n","jardin otanique\n","----------------\n","cem hann\n","ceem hann\n","----------------\n","hôpital youssou mbargane diop\n","hôpital youssou mbargan diop\n","----------------\n","boulevard général de gaule (centenaire)\n","boulevard général de geaule centenaire\n","----------------\n","equipe\n","equipus\n","----------------\n","pharmacie mame\n","pharmacie mamie\n","----------------\n","elton fks usine de farine\n","elton fks usine de farigne\n","----------------\n","brioche dorée maristes\n","brioche dorée majuises\n","----------------\n","yavuz collège bosphore\n","yavuz sédim-collège bosphore\n","----------------\n","sodida\n","soudida\n","----------------\n","western union marché ndiaréme\n","westen union marché ndiaréme\n","----------------\n","gare ferroviaire de dakar\n","gare feroviare de dakar\n","----------------\n","hlm patte d'oie\n","hlm patte 'oie\n","----------------\n","station d'epuration de cambéréne\n","station d'eburation de cambéréne\n","----------------\n","mbeubeuss\n","beubeuss\n","----------------\n","terminus parcelles terminus parcelles assainies\n","erminus parcelles terminus parcelles assainies\n","----------------\n","place de l’indépendance\n","place de l<unk>indépendance\n","----------------\n","ecole mamadou diagne ouakam\n","ecole mamatou diagne ouakam\n","----------------\n","cheikh anta diop avenue dakar\n","sheikh anta diop avenue<unk> dakar\n","----------------\n","danguou\n","tanguou\n","----------------\n","bd général de gaule sanitaire\n","bd général de gaule saintenaire\n","----------------\n","serigne ouakam\n","samerie ouakam\n","----------------\n","place de l’indépendance\n","place de l<unk>indépendance\n","----------------\n","centre de sauvegarde\n","centre de souvedade\n","----------------\n","attijari izdihar\n","hapijari lizdihar\n","----------------\n","universite dakar bourguiba diam\n","universite dakar bourgi dam\n","----------------\n","dominique nu\n","dominique n\n","----------------\n","poste courant rufisque\n","poste courante rufisque\n","----------------\n","pamecas cheikh wade\n","pamecas cheikh wad\n","----------------\n","sud rufisque\n","sd rufisque\n","----------------\n","dame laay jële oto de niary dem\n","darmen laay jële oto bide njiry dem\n","----------------\n","peulh pompier de rufisque\n","apeul pompier de rufisque\n","----------------\n","gare oncf\n","gare ancf\n","----------------\n","station shell de diockoul\n","station shell de diockouno\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-4eeb401febbd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m   \u001b[0mflag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0msim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdifflib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_close_matches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m       \u001b[0msim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/difflib.py\u001b[0m in \u001b[0;36mget_close_matches\u001b[0;34m(word, possibilities, n, cutoff)\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_seq1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreal_quick_ratio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mcutoff\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m            \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquick_ratio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mcutoff\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m            \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mratio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mcutoff\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mratio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/difflib.py\u001b[0m in \u001b[0;36mquick_ratio\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m                 \u001b[0mnumb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfullbcount\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m             \u001b[0mavail\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0melt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumb\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnumb\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m                 \u001b[0mmatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatches\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"ioRo-ZXo_A5W","executionInfo":{"status":"ok","timestamp":1619034642064,"user_tz":-120,"elapsed":922,"user":{"displayName":"Roman Engeler","photoUrl":"","userId":"01825020783678319162"}}},"source":["from nltk.probability import FreqDist\n","from wordcloud import WordCloud, ImageColorGenerator\n","\n","#\n","words = df['transcription']\n","allwords = []\n","\n","for wordlist in words:\n","  allwords += list(wordlist.lower().split())\n","\n","# histogram\n","mostcommon_small = FreqDist(allwords).most_common(10000)\n","xv, yv = zip(*mostcommon_small)"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PfpSuShDp7xt","executionInfo":{"status":"ok","timestamp":1618760735888,"user_tz":-120,"elapsed":610,"user":{"displayName":"Roman Engeler","photoUrl":"","userId":"01825020783678319162"}},"outputId":"62d90fab-59fb-415b-863e-d84dd04cdf13"},"source":["wer_metric.compute(predictions=['hann bel-air'], references=['hann bel - air'])"],"execution_count":227,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.75"]},"metadata":{"tags":[]},"execution_count":227}]},{"cell_type":"code","metadata":{"id":"ewoUsN39ghU7"},"source":["0.07964882287993731"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z352pFrf1CSN"},"source":["### Language Model"]},{"cell_type":"code","metadata":{"id":"D7l35GDh3AHc"},"source":["# to be considered\n","# model produces output of the form (CTC)\n","# <pad> <pad> <pad> <pad> <pad> <pad> r <pad> <pad> o u u <pad> <pad> t <pad> <pad> e <pad>\n","# => do we perform beam search on this sequence or first clean up the <pad> tokens?"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OgboLRG43Hpp"},"source":["#### Beam Search"]},{"cell_type":"code","metadata":{"id":"Oh7Hwn5g3AEx","executionInfo":{"status":"ok","timestamp":1618744538584,"user_tz":-120,"elapsed":183800,"user":{"displayName":"Roman Engeler","photoUrl":"","userId":"01825020783678319162"}}},"source":["# Beam Search\n","# https://towardsdatascience.com/boosting-your-sequence-generation-performance-with-beam-search-language-model-decoding-74ee64de435a\n","\n","import math\n","\n","def beam_search_decoder(predictions, top_k = 3):\n","    #start with an empty sequence with zero score\n","    output_sequences = [([], 0)]\n","    \n","    #looping through all the predictions\n","    for token_probs in predictions:\n","        new_sequences = []\n","        \n","        #append new tokens to old sequences and re-score\n","        for old_seq, old_score in output_sequences:\n","            for char_index in range(len(token_probs)):\n","                new_seq = old_seq + [char_index]\n","                #considering log-likelihood for scoring\n","                new_score = old_score + math.log(token_probs[char_index])\n","                new_sequences.append((new_seq, new_score))\n","                \n","        #sort all new sequences in the de-creasing order of their score\n","        output_sequences = sorted(new_sequences, key = lambda val: val[1], reverse = True)\n","        \n","        #select top-k based on score \n","        # *Note- best sequence is with the highest score\n","        output_sequences = output_sequences[:top_k]\n","        \n","    return output_sequences"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"80Mhxc6wpAW-","executionInfo":{"status":"ok","timestamp":1618744589863,"user_tz":-120,"elapsed":509,"user":{"displayName":"Roman Engeler","photoUrl":"","userId":"01825020783678319162"}},"outputId":"8d38795f-adf7-4e7f-b205-6f02f3e509e3"},"source":["\n","# test beam search \n","idx = 10\n","nbeams = 10\n","softmax = nn.Softmax(dim=2)\n","\n","#\n","pred = []\n","input_dict = df_train['audio_signal'][idx:idx+1].apply(prepare_dataset)\n","\n","for idx in range(len(input_dict)):\n","  #print('-----------------')\n","  logits = XLSRmodel(input_dict.values[idx].input_values.to(\"cuda\")).logits\n","  # sum_j(output_ij) = 1 where i is column and j is row\n","  output = softmax(logits) # logits -> probabilities\n","\n","  beams_int = beam_search_decoder(torch.squeeze(output).tolist(), top_k = nbeams) # beams\n","  beams_str = nbeams*['']\n","\n","  for k in range(nbeams):\n","    pred_ids, pred_prob = beams_int[k]\n","    print(processor.decode(pred_ids))"],"execution_count":11,"outputs":[{"output_type":"stream","text":["marché yeumbeul laa bëgg dem\n","marché yeumbeul laa bëgg dem\n","marché yeumbeul laa bëgg dem\n","marché yeumbeul laa bëgg dem\n","marché yeumbeul laa bëgg dem\n","marché yeumbeul laa bëgg dem\n","marché yeumbeul laa bëgg dem\n","marché yeumbeul laa bëgg dem\n","marché yeumbeul laa bëgg dem\n","marché yeumbeul laa bëgg dem\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"I3F24ZyA3LQ9"},"source":["#### Language Model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"NZ2OOznq3AC7","executionInfo":{"status":"ok","timestamp":1618744590180,"user_tz":-120,"elapsed":466,"user":{"displayName":"Roman Engeler","photoUrl":"","userId":"01825020783678319162"}},"outputId":"3e8fd50d-5809-40b1-fb65-a87cddafb421"},"source":["# need to find optimal n of n-gram\n","\n","## Language Model (n-gram vs KenLM)\n","# https://surfertas.github.io/deeplearning/pytorch/2017/08/20/n-gram.html # pytorch code (NN parametrization of LM)\n","# https://web.stanford.edu/~jurafsky/slp3/slides/LM_4.pdf\n","# https://www.kaggle.com/alvations/n-gram-language-model-with-nltk # code taken from here\n","# https://web.stanford.edu/~jurafsky/slp3/old_oct19/3.pdf -> improvements to LM's\n","\n","df_lm = df_train[:5]\n","\n","from nltk.util import ngrams, bigrams\n","from nltk.lm.preprocessing import padded_everygram_pipeline\n","\n","from nltk.lm import MLE\n","\n","# padding\n","from nltk.lm.preprocessing import pad_both_ends\n","#print(list(pad_both_ends(df_lm['transcription'].values[0], n=2)))\n","#print(list(bigrams(pad_both_ends(df_lm['transcription'].values[0], n=2))))\n","\n","'''\n","# materialize\n","for ngramlize_sent in train_data:\n","    print(list(ngramlize_sent))\n","    print()\n","print('#############')\n","list(padded_sents)\n","'''"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"\\n# materialize\\nfor ngramlize_sent in train_data:\\n    print(list(ngramlize_sent))\\n    print()\\nprint('#############')\\nlist(padded_sents)\\n\""]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"cD79x0G12__v","executionInfo":{"status":"ok","timestamp":1618752332481,"user_tz":-120,"elapsed":527,"user":{"displayName":"Roman Engeler","photoUrl":"","userId":"01825020783678319162"}}},"source":["def train_ngram(LMmodel, data):\n","  '''\n","  input: model, list of sentences\n","  output: trained model\n","  '''\n","\n","  # one long string of words\n","  word_string = ' '.join(data)\n","\n","  # one long list of words\n","  word_list = word_string.split(' ')\n","\n","  # lower casing\n","  word_list_lower = [list(map(str.lower, [word]))[0]\n","                     for word in word_list]\n","\n","  # preprocess for language model\n","  train_data, padded_words = padded_everygram_pipeline(2, word_list_lower)\n","  \n","  # fit model\n","  LMmodel.fit(train_data, padded_words)\n","\n","  return LMmodel"],"execution_count":124,"outputs":[]},{"cell_type":"code","metadata":{"id":"dp21UrStLtIt","executionInfo":{"status":"ok","timestamp":1618756758112,"user_tz":-120,"elapsed":485,"user":{"displayName":"Roman Engeler","photoUrl":"","userId":"01825020783678319162"}}},"source":["# WORD LEVEL LANGUAGE MODEL ################################################\n","def train_ngram(LMmodel, data):\n","  '''\n","  input: model, list of sentences\n","  output: trained model\n","  '''\n","  #\n","  sentence_list = [sentence for sentence in data]\n","\n","  # lower casing\n","  word_list_lower = [[word.lower() for word in sentence.split(' ')] for sentence in sentence_list]\n","\n","  # preprocess for language model\n","  train_data, padded_words = padded_everygram_pipeline(3, word_list_lower)\n","  \n","  # fit model\n","  LMmodel.fit(train_data, padded_words)\n","\n","  return LMmodel"],"execution_count":209,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":69},"id":"YI3YS3j72_9O","executionInfo":{"status":"ok","timestamp":1618756761787,"user_tz":-120,"elapsed":1074,"user":{"displayName":"Roman Engeler","photoUrl":"","userId":"01825020783678319162"}},"outputId":"2dc38f63-7334-4057-fb46-ca985c62cb3f"},"source":["## language model\n","# IMPORTANT there seems to be a missmatch in the vocabulary (44 vs 49 chars)\n","# -> could lead to language model not knowing the character\n","LMmodel = MLE(3) # Lets train a n-gram model\n","LMmodel = train_ngram(LMmodel, df_train['transcription'].values)\n","print(LMmodel.vocab)\n","print(len(tokenizer.get_vocab()))\n","\n","'''\n","print(LMmodel.counts['c'])\n","print(LMmodel.counts[['c']]['o'])  # P('o'|'c')\n","print(LMmodel.score('o', ['c']))\n","print(LMmodel.vocab.lookup([char for char in test_lower[5]]))\n","'''"],"execution_count":210,"outputs":[{"output_type":"stream","text":["<Vocabulary with cutoff=1 unk_label='<UNK>' and 783 items>\n","49\n"],"name":"stdout"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"\\nprint(LMmodel.counts['c'])\\nprint(LMmodel.counts[['c']]['o'])  # P('o'|'c')\\nprint(LMmodel.score('o', ['c']))\\nprint(LMmodel.vocab.lookup([char for char in test_lower[5]]))\\n\""]},"metadata":{"tags":[]},"execution_count":210}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r06wXfQlqmEV","executionInfo":{"status":"ok","timestamp":1618753448471,"user_tz":-120,"elapsed":507,"user":{"displayName":"Roman Engeler","photoUrl":"","userId":"01825020783678319162"}},"outputId":"a4b9d075-bdfe-402a-c1dd-29ca8da4812a"},"source":["# voabulary of language model (extracted from data)\n","print([ch for ch in LMmodel.vocab])\n","print(\"VERY STRANGE THAT THERE IS A c-cedi IN THE VOCABULARY EXTRACTED FROM THE TRAIN DATASET\")"],"execution_count":160,"outputs":[{"output_type":"stream","text":["['<s>', 'rufsac', '</s>', 'pharmacie', 'talibou', 'dabo', 'avenue', 'faidherbe', 'cité', 'mére', 'thérésa', 'gare', 'de', 'thiaroye', 'rue', 'baffa', 'séne', 'double', 'less', 'grande', 'mosquée', 'derkle', 'thokho', 'tournalou', 'yeumbeul', 'marché', 'laa', 'bëgg', 'dem', 'sonadis', 'rufisque', 'sococim', 'depot', 'layousse', 'faouzy', 'grand', 'dakar', 'fann', 'hock', 'canada', 'taly', 'bu', 'makk', 'pont', 'colobane', 'garage', 'camion', 'vidange', 'hopital', 'jean', 'la', 'fontaine', 'mariste', 'lamine', 'gueye', 'croisement', 'keur', 'massar', 'essence', 'touré', 'comico', 'darou', 'salam', 'parc', 'forestier', 'hann', 'massalikoul', 'jinan', 'ecobank', 'des', 'far', 'dama', 'mame', 'sira', 'ban', 'oto', 'mooy', 'jaar', 'yoff', 'yarakh', 'malicka', 'champ', 'course', 'pikine', 'seydina', 'limamoulaye', 'avenue,', 'edk', 'oil', 'ali', 'baba', 'rond', 'point', 'mbao', 'diaxay', 'lycée', 'thierno', 'seydou', 'nourou', 'tall', 'petit', 'extension', 'bountou', 'ecole', 'les', 'pédagogues', 'police', 'parcelles', 'assainies', 'car', 'bank', 'of', 'africa', 'nan', 'laay', 'def', 'ngir', 'boulangerie', 'jaune', 'mamelles', 'tivaoune', 'peulh', 'fadia', 'orabank', 'sagef', 'elimanel', 'fall', 'thiakhogne', 'mermoz', 'dékh', '(fleuve)', 'diacksao', 'terrain', 'nation', 'unies', 'zone', 'b', 'hlm', 'ndiaréme', 'thiossane', 'dieupeul', 'elhadji', 'mansour', 'sy', 'bus', 'danguou', 'lpa', 'asecna', 'ouakam', 'ferroviaire', 'station', 'shell', 'cambérène', 'port', 'sandiniery', 'echangeur', 'routiére', 'beaux', 'maraichiers', 'elton', 'ouest', 'foire', 'clinique', 'du', 'cap', 'manuel', 'gendarmerie', 'diall', \"m'baye\", 'camp', 'sékou', 'mballo', 'terminus', 'frigo', 'niary', 'tally', 'stade', 'seffa', 'ada', 'seck', 'centre', 'electrique', 'kounoune', 'tigo', 'almadies', 'embarcadére', 'gorée', 'sipres', 'al', 'azhar', 'sips', 'cimetiere', 'bétoir', 'niaye', 'khar', 'yalla', 'p', 'a', 'i', 'cimetiére', 'musulmane', 'route', 'hydrocarbures', 'lobat', 'mairie', 'lycee', 'blaise', 'diagne', 'nave', 'poste', 'courant', 'xarit', 'couture', 'sicap', 'baobab', 'niague', 'amadou', 'barry', 'bousso', 'dramé', 'notaire', 'total', 'médina', 'gounass', 'ba', 'bou', 'bess', 'cfao', 'motors', 'senegal', 'diaxxay', 'eaux', 'diockoul', 'just', 'for', 'u', 'sud', 'passage', 'icotaf', 'boulevard', 'gueule', 'tapée', 'hance', 'bernard', 'sde', 'axa', 'assurance', 'place', \"l'independance\", 'gouy', 'gui', 'diakayy', 'sonatel', 'zac', 'ics', 'béss', 'transfusion', 'sanguine', 'central', 'zinc', 'soboa', 'avion', 'cheikh', 'ahmadou', 'bamba', 'mbacke', 'souvenir', 'africain', 'feu', 'rouge', 'brioche', 'dorée', 'diamalaye', 'ville', 'sedami', 'nord', 'serigne', \"m'backé\", 'maternité', 'ndiarème', 'limamou', 'laye', 'tacko', 'ngor', 'cite', 'groupe', 'honda', 'ndaw', 'rts', 'las', 'palmas', 'le', 'baol', 'cyrnos', 'zoologique', 'rail', 'bi', 'fass', 'guediawaye', 'cem', 'joseph', 'felix', 'corréa', 'camberéne', \"d'akor\", 'assemblée', 'dieu', 'baobabs-', 'temple', 'nations', 'elementaire', 'iba', 'tali', 'bargny', 'khourounar', 'usine', 'méche', 'darling', 'dalal', 'diam', 'nganladou', 'diouf', 'mbeubeuss', 'diallo', 'tournal', 'assane', 'capec', 'niakoul', 'rap', 'nandos', 'baye', 'niasse', 'abass', 'ndao', 'benn', 'barack', 'menuserie', 'ebéniste', 'lamp', 'diop', 'nationale', 'dioutiba', 'sacré', 'coeur', 'elisabeth', 'l’indépendance', 'castor', 'bel', '-', 'air', 'routière', 'pompier', 'oilibya', 'peytavin', 'douane', 'golf', 'océan', 'guédiawaye', 'eric', 'kayser', 'front', 'terre', 'dior', 'radio', 'futurs', 'media', '(rfm)', 'coisement', 'cimetière', 'universite', 'hampate', 'supdeco', 'campus', 'e', 'mbedou', 'académia', 'sapeur', \"l'obelisque\", 'ndunkou', 'mbaye', 'yengoulene', 'castors', 'anta', 'marie', 'sarr', 'arret', 'patte', \"d'oie\", 'boune', 'tapé', 'alassane', 'djigo', 'alioune', 'sow', 'thioub', 'biagui', 'chambre', 'commerce', 'orca', 'escoa', 'pompiers', 'universitaire', 'soumbédioune', 'collège', 'cœur', 'mamadou', 'kakatar', 'par', 'dupont', 'et', 'demba', 'bceao', 'samu', 'municipale', 'grand-yoff', 'basket', 'principal', 'technopole', 'marchée', 'tandem', 'immobilier', 'scat', 'urbam', 'bissap', 'routiere', 'case', 'philipe', 'maguiléne', 'senghor', '(', ')', 'x', '(mermoz', 'virage', 'lac', 'rose', 'ndiaye', 'enseignent', 'doudou', 'basse', 'agence', 'immo', 'mbengue', 'sovonel', 'sandaga', 'roi', 'baudouin', 'ponty', 'djily', 'baobabs', 'medina', 'etoile', 'albert', 'sarraut', 'nu', 'may', 'assemblee', 'palais', 'presidentiel', 'femme', 'auto', 'santhiaba', 'pressafrik', 'cafétéria', 'creations', 'yavuz', 'sélim', 'bosphore', \"l'emergence\", 'mbédou', 'lébous', 'mbénguéne', 'carrapide', 'dépot', 'dikk', 'sapeurs', 'bachir', 'saint', 'pierre', 'fks', 'farine', 'western', 'union', 'cinéma', 'pasteur', 'dial', 'bass', 'dalifort', 'bd', 'général', 'gaule', '(centenaire)', 'mouride', 'el', 'hadji', 'ibrahima', 'pénitence', 'daroukhane', 'bonnet', 'autoroute', 'seydina-limamoulaye', 'diarra', 'marechal', 'ndox', 'yaye', 'allées', 'cheikhna', 'sidaty', 'aîdara', 'penitence', 'justice', 'tableau', 'ferrailes', 'théâtre', 'national', 'malika', 'mangazin', 'jardin', 'botanique', 'primaire', 'bén', 'camberene', 'niass', 'malick', 'ancien', 'lgi', 'bem', 'aeroport', 'ministère', 'santé', 'prévention', 'aéroport', 'buiscuterie', 'kappa', 'galandou', 'supermarché', 'machallah', 'maristes', 'mar', 'enseignant', 'daral', 'ndax', 'mën', 'naa', 'jël', 'zola', 'cours', 'sainte', 'wade', 'martyr', \"l'ouganda\", 'derklé', 'penitance', 'emg', 'automobile', 'téne', 'parfumerie', 'gandour', 'village', 'art', 'sahm', 'poisson', 'biches', 'guigon', 'saveurs', \"d'asie\", 'sherif', 'ouseynou', 'thiaw', 'orange', 'face', 'contenaire', 'district', 'sanitaire', 'est', 'sauvegarde', 'hôpital', 'cto', 'feroviére', 'issa', 'cambéréne', 'scolaire', 'gaston', 'berger', 'grands', 'moulins', 'université', 'virtuel', 'sénégal', 'sultan', 'prefecture', 'lo', 'bira', 'complexe', 'xelcom', 'bache', 'ly-mo-dac', 'casino', 'vert', 'senelec', 'fan', 'jële', 'sedima', 'restaurant', 'venisia', 'parcelle', 'marchande', 'saf', 'bar', 'batrain', 'fi', 'sportiff', 'pamecas', 'cms', 'clando', 'daniel', 'sorano', 'hospitalier', 'aristide', 'dantec', 'difoncé', 'militaire', 'comercial', 'plateau', 'dialoré', 'plage', 'liberté', 'socio', 'culturel', 'wakhinane', 'nimzatt', 'claudel', \"d'epuration\", 'douta', 'baraka', 'oncf', 'demeure', 'caesar', 'stadium', 'marius', 'notre', 'dame', 'fastef', 'commissariat', 'dieuppeul', 'vdn', 'péage', 'mbenguéne', 'guinaw', 'rails', 'kennedy', 'kaki', 'institution', 'immaculée', 'conception', 'karack', 'ndiakhirate', 'sandicat', 'nabil', 'choucair', 'imprimerie', 'tandjan', 'sgbs', 'hotel', 'terrou', 'canal', 'dagoudane', 'ndoyéne', 'lébou', 'béthio', 'acapes', 'mbor', 'rouidate', 'thiane', 'ngogne', 'birane', 'ly', 'cosmetique', 'fallou', 'papa', 'guèye', 'tiléne', 'texaco', 'petersen', 'sodida', 'ndeureuhlou', 'club', 'ministre', 'khonkh', '(station', 'clean', 'oil)', 'fahd', 'ben', 'abdel', 'aziz,', 'normale', 'supérieure', 'ningala', 'adji', 'niagna', 'dispensaire', 'norade', 'diamaguéne', 'maurice', 'delafosse', 'dominique', 'ministere', \"l'interieur\", 'senegalais', 'youssou', 'mbargane', 'ravin', 'sica', 'thiakhane', 'lat', 'flag', 'diarri', 'poul', 'medine', 'lazzare', 'privée', '\"les', 'petits', 'génies\"', 'christa', 'talli', 'mourade', 'mbacké', 'mamelle', 'jet', \"d'eau\", 'awa', 'fann,', 'sante', 'gaspar', 'camara', 'cices', 'magic', 'land', \"l'obélisque\", 'saldia', 'mosquee', 'mbeur', 'abiya', 'kenedy', 'fu', 'mu', 'jëm', 'abebe', 'bikila,', 'sengor', 'sofrac', 'pour', 'rokhaya', 'attijari', 'izdihar', 'bourguiba', 'poind', 'africatel', 'avs', 'diaakay', 'fouta', 'hôtel', 'diouma', 'leclerc', 'sotiba', 'hyacinthe', 'thiandoum', 'safco', 'caserne', 'samba', 'diéri', 'français', 'intersection', 'raby', 'equipe', 'plus', '(sur', 'mer)', 'nioul', 'cge', '<UNK>']\n","VERY STRANGE THAT THERE IS A c-cedi IN THE VOCABULARY EXTRACTED FROM THE TRAIN DATASET\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g1_X0PHppglm","executionInfo":{"status":"ok","timestamp":1618751443447,"user_tz":-120,"elapsed":564,"user":{"displayName":"Roman Engeler","photoUrl":"","userId":"01825020783678319162"}},"outputId":"f5956e79-cddf-4678-91c7-74c7e59f1f7d"},"source":["# vocabulary used by tokenizer (french alphabet)\n","vocab_dict = {v for k, v in enumerate(tokenizer.get_vocab())}\n","vocab_tokenizer = [v.lower() for v in vocab_dict]\n","print(vocab_tokenizer)"],"execution_count":99,"outputs":[{"output_type":"stream","text":["['ê', 'j', 'z', 'd', 'f', 's', 'ÿ', 'u', \"'\", 'à', 'h', 'û', '<pad>', 'b', 'a', 'â', 'y', 'î', 'i', 'r', '</s>', 'p', 'è', '-', 'w', 'é', '<unk>', 'g', 'ù', 'æ', 'q', 'o', 'ç', '<s>', '|', 'x', 'm', 'ô', 'n', 'c', 'k', 'l', 'œ', 'ü', 'ë', 'e', 't', 'ï', 'v']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_qvwRJ8l3Xiy"},"source":["#### Pipeline"]},{"cell_type":"code","metadata":{"id":"GJIQfTGZ2_6w","executionInfo":{"status":"ok","timestamp":1618752347970,"user_tz":-120,"elapsed":760,"user":{"displayName":"Roman Engeler","photoUrl":"","userId":"01825020783678319162"}}},"source":["# perplexity to acount for longer sequences\n","def ngram_logprobability(sentence):\n","    log_prob = 0\n","    count = 0\n","    for words in sentence:\n","      for ngram in words:\n","        # to avoid log(0) for unknown chars => many methods exist in the literature such as smoothing\n","        # since log is monotonically increasing, adding a const should not change the ordering, right?\n","        log_prob += np.log(LMmodel.score(ngram[1], [ngram[0]])+ 1e-8)\n","        count += 1\n","    return np.power(np.exp(log_prob), 1/count) # (inverse) perplexity to account for different word/ sentence length"],"execution_count":126,"outputs":[]},{"cell_type":"code","metadata":{"id":"LS2BR_kkOd7a","executionInfo":{"status":"ok","timestamp":1618756795566,"user_tz":-120,"elapsed":545,"user":{"displayName":"Roman Engeler","photoUrl":"","userId":"01825020783678319162"}}},"source":["# WORD LEVEL LANGUAGE MODEL ################################################\n","def ngram_logprobability(sentence):\n","    log_prob = 0\n","    count = 0\n","    for ngram in sentence:\n","      # to avoid log(0) for unknown chars => many methods exist in the literature such as smoothing\n","      # since log is monotonically increasing, adding a const should not change the ordering, right?\n","      log_prob += np.log(LMmodel.score(ngram[2], [ngram[0], ngram[1]])+ 1e-8)\n","      count += 1\n","    return np.power(np.exp(log_prob), 1/count) # (inverse) perplexity to account for different word/ sentence length"],"execution_count":214,"outputs":[]},{"cell_type":"code","metadata":{"id":"R_tgrLoB2_31","executionInfo":{"status":"ok","timestamp":1618756263554,"user_tz":-120,"elapsed":479,"user":{"displayName":"Roman Engeler","photoUrl":"","userId":"01825020783678319162"}}},"source":["def logprob_sentences(sentences):\n","  '''\n","  input: list of sentences\n","  output: log probability for sentences\n","  '''\n","  # list with log probabilities\n","  log_probs = len(sentences)* [-np.infty]\n","\n","  # creating list of sentences from string\n","  list_sentences = [sentence.split(' ') for sentence in sentences]\n","\n","  # lower casing\n","  for k in range(len(list_sentences)):\n","    list_sentences[k] = [list(map(str.lower, [sent]))[0]\n","                        for sent in list_sentences[k]]\n","\n","  # list(sentence_list(word_list(ngrams)))\n","  list_ngrams = [[list(ngrams(pad_both_ends(word, n=2), n=2)) for word in sentence] for sentence in list_sentences]\n","\n","  for k, sentence in enumerate(list_ngrams):\n","    log_probs[k] = ngram_logprobability(sentence)\n","\n","  return log_probs"],"execution_count":200,"outputs":[]},{"cell_type":"code","metadata":{"id":"kWyFoNTLOn6o","executionInfo":{"status":"ok","timestamp":1618756799230,"user_tz":-120,"elapsed":525,"user":{"displayName":"Roman Engeler","photoUrl":"","userId":"01825020783678319162"}}},"source":["# WORD LEVEL LANGUAGE MODEL ################################################\n","def logprob_sentences(sentences):\n","  '''\n","  input: list of sentences\n","  output: log probability for sentences\n","  '''\n","\n","  '''\n","  #\n","  sentence_list = [sentence for sentence in data]\n","\n","  # lower casing\n","  word_list_lower = [[word.lower() for word in sentence.split(' ')] for sentence in sentence_list]\n","\n","  # preprocess for language model\n","  train_data, padded_words = padded_everygram_pipeline(2, word_list_lower)\n","  '''\n","\n","  # list with log probabilities\n","  log_probs = len(sentences)* [-np.infty]\n","\n","  # creating list of sentences from string\n","  list_sentences = [sentence.split(' ') for sentence in sentences]\n","\n","  # lower casing\n","  for k in range(len(list_sentences)):\n","    list_sentences[k] = [word.lower() for word in list_sentences[k]]\n","\n","  # list(sentence_list(word_list(ngrams)))\n","  list_ngrams = [list(ngrams(pad_both_ends(sentence, n=3), n=3)) for sentence in list_sentences]\n","\n","  for k, sentence in enumerate(list_ngrams):\n","    log_probs[k] = ngram_logprobability(sentence)\n","\n","  return log_probs"],"execution_count":215,"outputs":[]},{"cell_type":"code","metadata":{"id":"9tBUrQob5MWK","executionInfo":{"status":"ok","timestamp":1618753932260,"user_tz":-120,"elapsed":478,"user":{"displayName":"Roman Engeler","photoUrl":"","userId":"01825020783678319162"}}},"source":["## Unit Tests"],"execution_count":171,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CTHSNpfq88_H","executionInfo":{"status":"ok","timestamp":1618753723333,"user_tz":-120,"elapsed":495,"user":{"displayName":"Roman Engeler","photoUrl":"","userId":"01825020783678319162"}},"outputId":"6ba5418e-1995-4ce6-a1f0-9a5ffb1b884f"},"source":["df_lm['transcription'].values"],"execution_count":164,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['Rufsac', 'Pharmacie Talibou Dabo', 'Avenue Faidherbe',\n","       'Cité mére Thérésa', 'Gare de Thiaroye'], dtype=object)"]},"metadata":{"tags":[]},"execution_count":164}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eZ8j_BQvUQUd","executionInfo":{"status":"ok","timestamp":1618753757722,"user_tz":-120,"elapsed":543,"user":{"displayName":"Roman Engeler","photoUrl":"","userId":"01825020783678319162"}},"outputId":"8bec34eb-df94-4b87-e068-2250aaef05d0"},"source":["LMmodel.score('e', ['l', 'y', 'c'])"],"execution_count":166,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.21875"]},"metadata":{"tags":[]},"execution_count":166}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o2CjLTGP2_vm","executionInfo":{"status":"ok","timestamp":1618754485294,"user_tz":-120,"elapsed":599,"user":{"displayName":"Roman Engeler","photoUrl":"","userId":"01825020783678319162"}},"outputId":"87404a09-4b9b-4bc5-faa8-9a0675b74f42"},"source":["print(logprob_sentences(['lycée', 'lycee', 'lycèe']))\n","print(logprob_sentences(['lycée camp', 'lycee camp', 'lycèe camp']))"],"execution_count":191,"outputs":[{"output_type":"stream","text":["[9.371477854353765e-07, 6.7002595641296955e-06, 9.999999999999982e-09]\n","[0.0002041969179860819, 7.657042980709192e-07, 9.999999999999994e-09]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3AZaOoNn3ydn","executionInfo":{"status":"ok","timestamp":1618754506582,"user_tz":-120,"elapsed":488,"user":{"displayName":"Roman Engeler","photoUrl":"","userId":"01825020783678319162"}},"outputId":"acc10fa7-af90-4f23-f23a-7b45e8767b1c"},"source":["print(logprob_sentences(['keur', 'keur', 'keurr']))\n","print(logprob_sentences(['keur massar', 'keur masar', 'keurr massar']))"],"execution_count":192,"outputs":[{"output_type":"stream","text":["[1.3677197907879315e-06, 1.3677197907879315e-06, 9.999999999999982e-09]\n","[0.05430633274969011, 2.654593871608607e-07, 4.641588849084741e-06]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6yQsqSimClN_","executionInfo":{"status":"ok","timestamp":1618751924571,"user_tz":-120,"elapsed":7265,"user":{"displayName":"Roman Engeler","photoUrl":"","userId":"01825020783678319162"}}},"source":["#\n","input_dict = df_valid['audio_signal'].apply(prepare_dataset)"],"execution_count":120,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"81zQn2aG1EVU","executionInfo":{"status":"ok","timestamp":1618758520581,"user_tz":-120,"elapsed":1306354,"user":{"displayName":"Roman Engeler","photoUrl":"","userId":"01825020783678319162"}},"outputId":"cd6d92b2-0e1d-463a-f118-290e1f417abf"},"source":["# ATTENTION: XLSR model seems to be overconfident -> places all probability mass on one logit\n","## Predictions\n","nbeams = 50\n","\n","# word error rate\n","wer_ = []\n","\n","#\n","softmax = nn.Softmax(dim=2)\n","\n","## WER over everything (one long string)\n","label_str = ''\n","pred_str = ''\n","\n","for idx in range(len(df_valid)):\n","  #print('-----------------')\n","  logits = XLSRmodel(input_dict.values[idx].input_values.to(\"cuda\")).logits\n","  # sum_j(output_ij) = 1 where i is column and j is row\n","  output = softmax(logits) # logits -> probabilities\n","\n","  # beam search\n","  beams_int = beam_search_decoder(torch.squeeze(output).tolist(), top_k = nbeams) # beams\n","  beams_str = nbeams*['']\n","\n","  for k in range(nbeams):\n","    pred_ids, pred_prob = beams_int[k]\n","    beams_str[k] = processor.decode(pred_ids)\n","\n","  '''\n","  if beams_str[0] != beams_str[1]:\n","    print('--------------------')\n","    print(df_valid['ID'].values[idx])\n","    print(beams_str)\n","    print(logprob_sentences(beams_str))\n","    print(beams_str[np.argmax(logprob_sentences(beams_str))])\n","  '''\n","\n","  # prediction P(final) = Alpha * P(model) + Beta * P(L.M.)\n","  ## WER per sentence and then average\n","  #logprob = logprob_sentences(beams_str)\n","  #pred_str = beams_str[np.argmax(logprob)]\n","  #label_str = df_valid[\"transcription\"].values[idx].lower()\n","\n","  # need same length for wer_metric ? really\n","  #label_str = label_str.ljust(len(pred_str))\n","  #pred_str = pred_str.ljust(len(label_str))\n","\n","  ## WER over everything (one long string)\n","  pred_str+= beams_str[np.argmax(logprob_sentences(beams_str))]+ ' '\n","  \n","  label_str+= df_valid[\"transcription\"].values[idx].lower()+ ' '\n","\n","wer_.append(wer_metric.compute(predictions=[pred_str], references=[label_str]))\n","\n","print(np.mean(wer_))"],"execution_count":219,"outputs":[{"output_type":"stream","text":["0.05319896883056011\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uCplyao15gDu"},"source":["## spell checking\n","# 1) train LM as n-gram. prediction with model. run prediction through levenstein distance, use n-gram to vote\n","# 2) use pystellchecker -> only in French so far\n","# https://pypi.org/project/pyspellchecker/\n","# 3) textBlob \n","# https://stackabuse.com/spelling-correction-in-python-with-textblob/\n","# https://github.com/sloria/TextBlob"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nYfPuOkg8izK","executionInfo":{"status":"ok","timestamp":1618749023478,"user_tz":-120,"elapsed":485,"user":{"displayName":"Roman Engeler","photoUrl":"","userId":"01825020783678319162"}},"outputId":"0392bde5-c806-4cb2-b488-f7def3e7b814"},"source":["id= 'a8d8c6221854d1b721162a4ecfbdf87554e0b39c782ccd1914aeaddf3491a92df99ac7cee4264d5b031b0e779e1e64d7206deca98ea39009e579fb7cab164ffe'\n","\n","input_dict = df_valid[df_valid['ID']==id]['audio_signal'].apply(prepare_dataset)\n","logits = XLSRmodel(input_dict.values[0].input_values.to(\"cuda\")).logits\n","pred_ids = torch.argmax(logits, dim=-1)[0]\n","\n","print(processor.decode(pred_ids))"],"execution_count":70,"outputs":[{"output_type":"stream","text":["4390    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n","Name: audio_signal, dtype: object\n","[{'input_values': tensor([[-7.9619e-05, -7.9619e-05, -7.9619e-05,  ..., -7.9619e-05,\n","         -7.9619e-05, -7.9619e-05]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]])}]\n","sheikh anta diop avenue<unk> dakar\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9j6e3XwuSyhj"},"source":["# with word 3-gram\n","N=3 -> 0.07475978439184439\n","N=5 -> 0.07030700726505741\n","N=10 -> 0.06304194984766816\n","N=50 -> 0.05319896883056011 => 0.118 = 11.8% auf test satz\n","\n","# with word 2-gram\n","N=3 -> 0.07499414108272791\n","N=5 -> 0.07077572064682447\n","N=10 -> 0.06397937661120225\n","N=20 -> 0.059057886102648234\n","\n","# with word 1-gram\n","N=3 -> 0.07710335130067963\n","N=5 -> 0.07499414108272791\n","N=10 -> 0.07382235762831028"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OkcHVqNAm6Fp"},"source":["# with char 5-gram\n","N=3 -> 0.07780642137333021\n","N=5 -> 0.07569721115537849 down\n","N=10 ->0.07405671431919382 down\n","\n","# with char 4-gram\n","N=3 -> 0.07921256151863136\n","N=5 -> 0.0787438481368643 down\n","N=10 ->0.07850949144598078 down\n","\n","# with char 3-gram\n","N=1 -> 0.0824935551910007\n","N=2 -> 0.08202484180923365 down\n","N=3 -> 0.08366533864541832 up\n","N=5 -> 0.08577454886337005 up\n","N=80 ->0.14811342863838764 up\n","\n","# with char 2-gram\n","N=3 -> 0.08600890555425357\n","N=5 -> 0.09210217951722521 up\n","\n","# w/o\n","    -> 0.0824935551910007"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FtWwOM_31G1E"},"source":["### Prediction"]},{"cell_type":"code","metadata":{"id":"4cAyWHup1IGk","executionInfo":{"status":"ok","timestamp":1618758601357,"user_tz":-120,"elapsed":4350,"user":{"displayName":"Roman Engeler","photoUrl":"","userId":"01825020783678319162"}}},"source":["## prediction\n","# load data (dataframe) -> empty entries\n","df_test = pd.read_feather('drive/MyDrive/Colab Notebooks/data/ASR_test_audio1564.ft')\n","df_test = df_test[['ID', 'audio_signal']]\n","\n","df_test.head()\n","\n","def prepare_dataset(batch):\n","    return processor(batch, return_tensors=\"pt\", sampling_rate=16*1e3)"],"execution_count":220,"outputs":[]},{"cell_type":"code","metadata":{"id":"EWVuQMO9WoWL","executionInfo":{"status":"ok","timestamp":1618758608898,"user_tz":-120,"elapsed":8709,"user":{"displayName":"Roman Engeler","photoUrl":"","userId":"01825020783678319162"}}},"source":["#\n","input_dict = df_test['audio_signal'].apply(prepare_dataset)"],"execution_count":221,"outputs":[]},{"cell_type":"code","metadata":{"id":"h4tNbrEyNLnJ","executionInfo":{"status":"ok","timestamp":1618760285968,"user_tz":-120,"elapsed":1513040,"user":{"displayName":"Roman Engeler","photoUrl":"","userId":"01825020783678319162"}}},"source":["# run through processor\n","import difflib\n","#input_dict = df_test['audio_signal'].apply(prepare_dataset)\n","preds = []\n","\n","nbeams = 50\n","\n","#\n","softmax = nn.Softmax(dim=2)\n","\n","# run through model and decoder\n","for i in range(len(df_test)):\n","  logits = XLSRmodel(input_dict.values[i].input_values.to('cuda')).logits\n","  output = softmax(logits) # logits -> probabilities\n","  \n","  # beam search\n","  beams_int = beam_search_decoder(torch.squeeze(output).tolist(), top_k = nbeams) # beams\n","  beams_str = nbeams*['']\n","\n","  for k in range(nbeams):\n","    pred_ids, pred_prob = beams_int[k]\n","    beams_str[k] = processor.decode(pred_ids)\n","\n","  # prediction P(final) = Alpha * P(model) + Beta * P(L.M.)\n","  logprob = logprob_sentences(beams_str)\n","  pred_str = beams_str[np.argmax(logprob)]\n","\n","  preds.append(pred_str)\n","\n","  if i % 200 == 0:\n","    print('Sentence '+ str(i))\n","\n","# save as csv\n","dfpred = pd.DataFrame(list(zip(list(df_test['ID'].values), preds)), columns=['ID', 'transcription'])\n","dfpred.to_csv('./drive/MyDrive/Colab Notebooks/predictionsLM_18AprLM.csv', index=False)"],"execution_count":222,"outputs":[]},{"cell_type":"code","metadata":{"id":"lNQXxwR5NuGC"},"source":["from nltk.probability import FreqDist\n","from wordcloud import WordCloud, ImageColorGenerator\n","\n","#\n","words = df['transcription']\n","allwords = []\n","\n","for wordlist in words:\n","  allwords += list(wordlist.lower().split())\n","\n","# histogram\n","mostcommon_small = FreqDist(allwords).most_common(10000)\n","xv, yv = zip(*mostcommon_small)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cazQLYfATOr6"},"source":[""],"execution_count":null,"outputs":[]}]}