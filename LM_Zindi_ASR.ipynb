{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LM_Zindi_ASR.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Tw0Lqm5073z"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CktwOwj01mM"
      },
      "source": [
        "## path\n",
        "path = 'drive/MyDrive/Colab Notebooks/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAO8Kt4_1WlF"
      },
      "source": [
        "%%capture\n",
        "!pip install datasets # to use\n",
        "!pip install git+https://github.com/huggingface/transformers # to user huggingface transformer\n",
        "!pip install jiwer # for wer metric\n",
        "\n",
        "!pip install -U pip\n",
        "!pip install -U dill\n",
        "!pip install -U nltk==3.4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7iUidpl1aZ6"
      },
      "source": [
        "## load packages\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import os.path\n",
        "\n",
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2CTCTokenizer\n",
        "from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Processor\n",
        "\n",
        "from datasets import load_metric\n",
        "\n",
        "import librosa as lb\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from nltk.util import pad_sequence\n",
        "from nltk.util import ngrams, bigrams\n",
        "from nltk.lm.preprocessing import pad_both_ends\n",
        "from nltk.lm.preprocessing import flatten"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNqhX5Jf1lFh"
      },
      "source": [
        "# seeding\n",
        "random.seed(10)\n",
        "np.random.seed(10)\n",
        "torch.manual_seed(10)\n",
        "torch.cuda.manual_seed_all(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DoQppKuT1nst",
        "outputId": "a6d7163d-0f26-40fc-b074-e2654df97acc"
      },
      "source": [
        "## mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVB9MG970-Zx"
      },
      "source": [
        "### Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MILC_uop1nCg"
      },
      "source": [
        "## read into memory (small)\n",
        "df = pd.read_feather('drive/MyDrive/Colab Notebooks/data/ASR_train_audio6683.ft')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAoYH-gv1AA8"
      },
      "source": [
        "## train valid split\n",
        "df_train, df_valid = train_test_split(df, test_size=0.2, random_state=1234)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9ev0iGy1AyJ"
      },
      "source": [
        "### XLSR Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QC7SJwHPPaTC"
      },
      "source": [
        "## word corpus for nearest neighbor\n",
        "from nltk.probability import FreqDist\n",
        "from wordcloud import WordCloud, ImageColorGenerator\n",
        "\n",
        "#\n",
        "words = df_train['transcription']\n",
        "allwords = []\n",
        "\n",
        "for wordlist in words:\n",
        "  allwords += list(wordlist.lower().split())\n",
        "\n",
        "# histogram\n",
        "mostcommon_small = FreqDist(allwords).most_common(10000) # it has around 1000 distinct words -> 10k to be sure that all are included\n",
        "xv, yv = zip(*mostcommon_small)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQOfoGgp1ByH",
        "outputId": "f434c640-02ca-4983-b973-d2f586d1821f"
      },
      "source": [
        "## Re-evaluate performance of model\n",
        "\n",
        "import difflib\n",
        "# load XLSR model\n",
        "if not 'XLSRmodel' in globals():\n",
        "  print('Load model')\n",
        "  XLSRmodel = Wav2Vec2ForCTC.from_pretrained('./drive/MyDrive/Colab Notebooks/model/wav2vec2-large-xlsr-french-24Apr/checkpoint-1950/').to(\"cuda\")\n",
        "\n",
        "# load processor\n",
        "if not 'processor' in globals():\n",
        "  print('Load processor')\n",
        "  tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(\"facebook/wav2vec2-large-xlsr-53-french\")\n",
        "  feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=True)\n",
        "  processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
        "\n",
        "# prepare dataset\n",
        "def prepare_dataset(batch):\n",
        "    return processor(batch, return_tensors=\"pt\", sampling_rate=16*1e3)\n",
        "\n",
        "# word error rate\n",
        "wer_metric = load_metric(\"wer\")\n",
        "wer_ = []\n",
        "\n",
        "# model prediction\n",
        "model_valid = []\n",
        "# model + vocab prediction\n",
        "model_vocab_valid = []\n",
        "\n",
        "#\n",
        "input_dict = df_valid['audio_signal'].apply(prepare_dataset)\n",
        "\n",
        "## WER over everything (one long string)\n",
        "label_str = ''\n",
        "pred_str = ''\n",
        "\n",
        "for idx in range(len(df_valid)):\n",
        "  #print('-----------------')\n",
        "  logits = XLSRmodel(input_dict.values[idx].input_values.to(\"cuda\")).logits\n",
        "  pred_ids = torch.argmax(logits, dim=-1)[0]\n",
        "\n",
        "  '''\n",
        "  # This is the part with nearest neighbors\n",
        "  pred = []\n",
        "  for word in processor.decode(pred_ids).split(' '):\n",
        "    sim = difflib.get_close_matches(word, xv, n=1)\n",
        "    if sim == []:\n",
        "      sim = word\n",
        "    else:\n",
        "      sim = sim[0]\n",
        "    pred.append(sim)\n",
        "  \n",
        "  model_valid.append(processor.decode(pred_ids))\n",
        "  model_vocab_valid.append(' '.join(pred))\n",
        "\n",
        "  #print(\"Prediction:\")\n",
        "  pred_str+= ' '.join(pred)+ ' '\n",
        "  '''\n",
        "\n",
        "  ## WER over everything (one long string)\n",
        "  pred_str+= processor.decode(pred_ids)+ ' '\n",
        "  label_str+= df_valid[\"transcription\"].values[idx].lower()+ ' '\n",
        "\n",
        "wer_.append(wer_metric.compute(predictions=[pred_str], references=[label_str]))\n",
        "\n",
        "print(np.mean(wer_))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.06843215373798922\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z352pFrf1CSN"
      },
      "source": [
        "### Language Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7l35GDh3AHc"
      },
      "source": [
        "# to be considered\n",
        "# model produces output of the form (CTC)\n",
        "# <pad> <pad> <pad> <pad> <pad> <pad> r <pad> <pad> o u u <pad> <pad> t <pad> <pad> e <pad>\n",
        "# => do we perform beam search on this sequence or first clean up the <pad> tokens?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgboLRG43Hpp"
      },
      "source": [
        "#### Beam Search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oh7Hwn5g3AEx"
      },
      "source": [
        "# Beam Search\n",
        "# https://towardsdatascience.com/boosting-your-sequence-generation-performance-with-beam-search-language-model-decoding-74ee64de435a\n",
        "\n",
        "import math\n",
        "\n",
        "def beam_search_decoder(predictions, top_k = 3):\n",
        "    #start with an empty sequence with zero score\n",
        "    output_sequences = [([], 0)]\n",
        "    \n",
        "    #looping through all the predictions\n",
        "    for token_probs in predictions:\n",
        "        new_sequences = []\n",
        "        \n",
        "        #append new tokens to old sequences and re-score\n",
        "        for old_seq, old_score in output_sequences:\n",
        "            for char_index in range(len(token_probs)):\n",
        "                new_seq = old_seq + [char_index]\n",
        "                #considering log-likelihood for scoring\n",
        "                new_score = old_score + math.log(token_probs[char_index])\n",
        "                new_sequences.append((new_seq, new_score))\n",
        "                \n",
        "        #sort all new sequences in the de-creasing order of their score\n",
        "        output_sequences = sorted(new_sequences, key = lambda val: val[1], reverse = True)\n",
        "        \n",
        "        #select top-k based on score \n",
        "        # *Note- best sequence is with the highest score\n",
        "        output_sequences = output_sequences[:top_k]\n",
        "        \n",
        "    return output_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80Mhxc6wpAW-",
        "outputId": "3963b486-f94a-4cfb-b1cd-15bcf926c55f"
      },
      "source": [
        "\n",
        "# test beam search \n",
        "idx = 10\n",
        "nbeams = 10\n",
        "softmax = nn.Softmax(dim=2)\n",
        "\n",
        "#\n",
        "pred = []\n",
        "input_dict = df_train['audio_signal'][idx:idx+1].apply(prepare_dataset)\n",
        "\n",
        "for idx in range(len(input_dict)):\n",
        "  #print('-----------------')\n",
        "  logits = XLSRmodel(input_dict.values[idx].input_values.to(\"cuda\")).logits\n",
        "  # sum_j(output_ij) = 1 where i is column and j is row\n",
        "  output = softmax(logits) # logits -> probabilities\n",
        "\n",
        "  beams_int = beam_search_decoder(torch.squeeze(output).tolist(), top_k = nbeams) # beams\n",
        "  beams_str = nbeams*['']\n",
        "\n",
        "  for k in range(nbeams):\n",
        "    pred_ids, pred_prob = beams_int[k]\n",
        "    print(processor.decode(pred_ids))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "marché yeumbeul laa bëgg dem\n",
            "marché yeumbeul laa bëgg dem\n",
            "marché yeumbeul laa bëgg dem\n",
            "marché yeumbeul laa bëgg dem\n",
            "marché yeumbeul laa bëgg dem\n",
            "marché yeumbeul laa bëgg dem\n",
            "marché yeumbeul laa bëgg dem\n",
            "marché yeumbeul laa bëgg dem\n",
            "marché yeumbeul laa bëgg dem\n",
            "marché yeumbeul laa bëgg dem\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3F24ZyA3LQ9"
      },
      "source": [
        "#### Language Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "NZ2OOznq3AC7",
        "outputId": "a6a0fd5e-dccc-46b7-f581-ede0ed08bd3e"
      },
      "source": [
        "# need to find optimal n of n-gram\n",
        "\n",
        "## Language Model (n-gram vs KenLM)\n",
        "# https://surfertas.github.io/deeplearning/pytorch/2017/08/20/n-gram.html # pytorch code (NN parametrization of LM)\n",
        "# https://web.stanford.edu/~jurafsky/slp3/slides/LM_4.pdf\n",
        "# https://www.kaggle.com/alvations/n-gram-language-model-with-nltk # code taken from here\n",
        "# https://web.stanford.edu/~jurafsky/slp3/old_oct19/3.pdf -> improvements to LM's\n",
        "\n",
        "df_lm = df_train[:5]\n",
        "\n",
        "from nltk.util import ngrams, bigrams\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "\n",
        "from nltk.lm import MLE\n",
        "\n",
        "# padding\n",
        "from nltk.lm.preprocessing import pad_both_ends\n",
        "#print(list(pad_both_ends(df_lm['transcription'].values[0], n=2)))\n",
        "#print(list(bigrams(pad_both_ends(df_lm['transcription'].values[0], n=2))))\n",
        "\n",
        "'''\n",
        "# materialize\n",
        "for ngramlize_sent in train_data:\n",
        "    print(list(ngramlize_sent))\n",
        "    print()\n",
        "print('#############')\n",
        "list(padded_sents)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n# materialize\\nfor ngramlize_sent in train_data:\\n    print(list(ngramlize_sent))\\n    print()\\nprint('#############')\\nlist(padded_sents)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHOqo_tmDhta",
        "outputId": "d005c847-3a43-43ab-ef5d-4d90cb7af85b"
      },
      "source": [
        "import re\n",
        "line = 'dies ist ein, \"haha\" test vielleicht-auch nicht)'\n",
        "line = re.sub('[,()\"]', '', line)\n",
        "print(line)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dies ist ein haha test vielleicht-auch nicht\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cD79x0G12__v"
      },
      "source": [
        "def train_ngram(LMmodel, data):\n",
        "  '''\n",
        "  input: model, list of sentences\n",
        "  output: trained model\n",
        "  '''\n",
        "\n",
        "  # one long string of words\n",
        "  word_string = ' '.join(data)\n",
        "\n",
        "  # one long list of words\n",
        "  word_list = word_string.split(' ')\n",
        "\n",
        "  # lower casing\n",
        "  word_list_lower = [list(map(str.lower, [word]))[0]\n",
        "                     for word in word_list]\n",
        "\n",
        "  # preprocess for language model\n",
        "  train_data, padded_words = padded_everygram_pipeline(2, word_list_lower)\n",
        "  \n",
        "  # fit model\n",
        "  LMmodel.fit(train_data, padded_words)\n",
        "\n",
        "  return LMmodel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dp21UrStLtIt"
      },
      "source": [
        "# WORD LEVEL LANGUAGE MODEL ################################################\n",
        "def train_ngram(LMmodel, data):\n",
        "  '''\n",
        "  input: model, list of sentences\n",
        "  output: trained model\n",
        "  '''\n",
        "  #\n",
        "  sentence_list = [sentence for sentence in data]\n",
        "\n",
        "  # lower casing\n",
        "  word_list_lower = [[re.sub('[,()\"]', '', word.lower()) for word in sentence.split(' ')] for sentence in sentence_list]\n",
        "\n",
        "  # preprocess for language model\n",
        "  train_data, padded_words = padded_everygram_pipeline(3, word_list_lower)\n",
        "  \n",
        "  # fit model\n",
        "  LMmodel.fit(train_data, padded_words)\n",
        "\n",
        "  return LMmodel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "YI3YS3j72_9O",
        "outputId": "75cc9dbd-44fd-4ae4-a7a1-9b7ca18cca7b"
      },
      "source": [
        "## language model\n",
        "# IMPORTANT there seems to be a missmatch in the vocabulary (44 vs 49 chars)\n",
        "# -> could lead to language model not knowing the character\n",
        "LMmodel = MLE(3) # Lets train a n-gram model\n",
        "LMmodel = train_ngram(LMmodel, df_train['transcription'].values)\n",
        "print(LMmodel.vocab)\n",
        "print(len(tokenizer.get_vocab()))\n",
        "\n",
        "'''\n",
        "print(LMmodel.counts['c'])\n",
        "print(LMmodel.counts[['c']]['o'])  # P('o'|'c')\n",
        "print(LMmodel.score('o', ['c']))\n",
        "print(LMmodel.vocab.lookup([char for char in test_lower[5]]))\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<Vocabulary with cutoff=1 unk_label='<UNK>' and 776 items>\n",
            "49\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nprint(LMmodel.counts['c'])\\nprint(LMmodel.counts[['c']]['o'])  # P('o'|'c')\\nprint(LMmodel.score('o', ['c']))\\nprint(LMmodel.vocab.lookup([char for char in test_lower[5]]))\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r06wXfQlqmEV",
        "outputId": "995e24c1-645e-4a20-f4de-3d6e71999fcd"
      },
      "source": [
        "# voabulary of language model (extracted from data)\n",
        "print([ch for ch in LMmodel.vocab])\n",
        "print(\"VERY STRANGE THAT THERE IS A c-cedi IN THE VOCABULARY EXTRACTED FROM THE TRAIN DATASET\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<s>', 'rufsac', '</s>', 'pharmacie', 'talibou', 'dabo', 'avenue', 'faidherbe', 'cité', 'mére', 'thérésa', 'gare', 'de', 'thiaroye', 'rue', 'baffa', 'séne', 'double', 'less', 'grande', 'mosquée', 'derkle', 'thokho', 'tournalou', 'yeumbeul', 'marché', 'laa', 'bëgg', 'dem', 'sonadis', 'rufisque', 'sococim', 'depot', 'layousse', 'faouzy', 'grand', 'dakar', 'fann', 'hock', 'canada', 'taly', 'bu', 'makk', 'pont', 'colobane', 'garage', 'camion', 'vidange', 'hopital', 'jean', 'la', 'fontaine', 'mariste', 'lamine', 'gueye', 'croisement', 'keur', 'massar', 'essence', 'touré', 'comico', 'darou', 'salam', 'parc', 'forestier', 'hann', 'massalikoul', 'jinan', 'ecobank', 'des', 'far', 'dama', 'mame', 'sira', 'ban', 'oto', 'mooy', 'jaar', 'yoff', 'yarakh', 'malicka', 'champ', 'course', 'pikine', 'seydina', 'limamoulaye', 'edk', 'oil', 'ali', 'baba', 'rond', 'point', 'mbao', 'diaxay', 'lycée', 'thierno', 'seydou', 'nourou', 'tall', 'petit', 'extension', 'bountou', 'ecole', 'les', 'pédagogues', 'police', 'parcelles', 'assainies', 'car', 'bank', 'of', 'africa', 'nan', 'laay', 'def', 'ngir', 'boulangerie', 'jaune', 'mamelles', 'tivaoune', 'peulh', 'fadia', 'orabank', 'sagef', 'elimanel', 'fall', 'thiakhogne', 'mermoz', 'dékh', 'fleuve', 'diacksao', 'terrain', 'nation', 'unies', 'zone', 'b', 'hlm', 'ndiaréme', 'thiossane', 'dieupeul', 'elhadji', 'mansour', 'sy', 'bus', 'danguou', 'lpa', 'asecna', 'ouakam', 'ferroviaire', 'station', 'shell', 'cambérène', 'port', 'sandiniery', 'echangeur', 'routiére', 'beaux', 'maraichiers', 'elton', 'ouest', 'foire', 'clinique', 'du', 'cap', 'manuel', 'gendarmerie', 'diall', \"m'baye\", 'camp', 'sékou', 'mballo', 'terminus', 'frigo', 'niary', 'tally', 'stade', 'seffa', 'ada', 'seck', 'centre', 'electrique', 'kounoune', 'tigo', 'almadies', 'embarcadére', 'gorée', 'sipres', 'al', 'azhar', 'sips', 'cimetiere', 'bétoir', 'niaye', 'khar', 'yalla', 'p', 'a', 'i', 'cimetiére', 'musulmane', 'route', 'hydrocarbures', 'lobat', 'mairie', 'lycee', 'blaise', 'diagne', 'nave', 'poste', 'courant', 'xarit', 'couture', 'sicap', 'baobab', 'niague', 'amadou', 'barry', 'bousso', 'dramé', 'notaire', 'total', 'médina', 'gounass', 'ba', 'bou', 'bess', 'cfao', 'motors', 'senegal', 'diaxxay', 'eaux', 'diockoul', 'just', 'for', 'u', 'sud', 'passage', 'icotaf', 'boulevard', 'gueule', 'tapée', 'hance', 'bernard', 'sde', 'axa', 'assurance', 'place', \"l'independance\", 'gouy', 'gui', 'diakayy', 'sonatel', 'zac', 'ics', 'béss', 'transfusion', 'sanguine', 'central', 'zinc', 'soboa', 'avion', 'cheikh', 'ahmadou', 'bamba', 'mbacke', 'souvenir', 'africain', 'feu', 'rouge', 'brioche', 'dorée', 'diamalaye', 'ville', 'sedami', 'nord', 'serigne', \"m'backé\", 'maternité', 'ndiarème', 'limamou', 'laye', 'tacko', 'ngor', 'cite', 'groupe', 'honda', 'ndaw', 'rts', 'las', 'palmas', 'le', 'baol', 'cyrnos', 'zoologique', 'rail', 'bi', 'fass', 'guediawaye', 'cem', 'joseph', 'felix', 'corréa', 'camberéne', \"d'akor\", 'assemblée', 'dieu', 'baobabs-', 'temple', 'nations', 'elementaire', 'iba', 'tali', 'bargny', 'khourounar', 'usine', 'méche', 'darling', 'dalal', 'diam', 'nganladou', 'diouf', 'mbeubeuss', 'diallo', 'tournal', 'assane', 'capec', 'niakoul', 'rap', 'nandos', 'baye', 'niasse', 'abass', 'ndao', 'benn', 'barack', 'menuserie', 'ebéniste', 'lamp', 'diop', 'nationale', 'dioutiba', 'sacré', 'coeur', 'elisabeth', 'l’indépendance', 'castor', 'bel', '-', 'air', 'routière', 'pompier', 'oilibya', 'peytavin', 'douane', 'golf', 'océan', 'guédiawaye', 'eric', 'kayser', 'front', 'terre', 'dior', 'radio', 'futurs', 'media', 'rfm', 'coisement', 'cimetière', 'universite', 'hampate', 'supdeco', 'campus', 'e', 'mbedou', 'académia', 'sapeur', \"l'obelisque\", 'ndunkou', 'mbaye', 'yengoulene', 'castors', 'anta', 'marie', 'sarr', 'arret', 'patte', \"d'oie\", 'boune', 'tapé', 'alassane', 'djigo', 'alioune', 'sow', 'thioub', 'biagui', 'chambre', 'commerce', 'orca', 'escoa', 'pompiers', 'universitaire', 'soumbédioune', 'collège', 'cœur', 'mamadou', 'kakatar', 'par', 'dupont', 'et', 'demba', 'bceao', 'samu', 'municipale', 'grand-yoff', 'basket', 'principal', 'technopole', 'marchée', 'tandem', 'immobilier', 'scat', 'urbam', 'bissap', 'routiere', 'case', 'philipe', 'maguiléne', 'senghor', '', 'x', 'virage', 'lac', 'rose', 'ndiaye', 'enseignent', 'doudou', 'basse', 'agence', 'immo', 'mbengue', 'sovonel', 'sandaga', 'roi', 'baudouin', 'ponty', 'djily', 'baobabs', 'medina', 'etoile', 'albert', 'sarraut', 'nu', 'may', 'assemblee', 'palais', 'presidentiel', 'femme', 'auto', 'santhiaba', 'pressafrik', 'cafétéria', 'creations', 'yavuz', 'sélim', 'bosphore', \"l'emergence\", 'mbédou', 'lébous', 'mbénguéne', 'carrapide', 'dépot', 'dikk', 'sapeurs', 'bachir', 'saint', 'pierre', 'fks', 'farine', 'western', 'union', 'cinéma', 'pasteur', 'dial', 'bass', 'dalifort', 'bd', 'général', 'gaule', 'centenaire', 'mouride', 'el', 'hadji', 'ibrahima', 'pénitence', 'daroukhane', 'bonnet', 'autoroute', 'seydina-limamoulaye', 'diarra', 'marechal', 'ndox', 'yaye', 'allées', 'cheikhna', 'sidaty', 'aîdara', 'penitence', 'justice', 'tableau', 'ferrailes', 'théâtre', 'national', 'malika', 'mangazin', 'jardin', 'botanique', 'primaire', 'bén', 'camberene', 'niass', 'malick', 'ancien', 'lgi', 'bem', 'aeroport', 'ministère', 'santé', 'prévention', 'aéroport', 'buiscuterie', 'kappa', 'galandou', 'supermarché', 'machallah', 'maristes', 'mar', 'enseignant', 'daral', 'ndax', 'mën', 'naa', 'jël', 'zola', 'cours', 'sainte', 'wade', 'martyr', \"l'ouganda\", 'derklé', 'penitance', 'emg', 'automobile', 'téne', 'parfumerie', 'gandour', 'village', 'art', 'sahm', 'poisson', 'biches', 'guigon', 'saveurs', \"d'asie\", 'sherif', 'ouseynou', 'thiaw', 'orange', 'face', 'contenaire', 'district', 'sanitaire', 'est', 'sauvegarde', 'hôpital', 'cto', 'feroviére', 'issa', 'cambéréne', 'scolaire', 'gaston', 'berger', 'grands', 'moulins', 'université', 'virtuel', 'sénégal', 'sultan', 'prefecture', 'lo', 'bira', 'complexe', 'xelcom', 'bache', 'ly-mo-dac', 'casino', 'vert', 'senelec', 'fan', 'jële', 'sedima', 'restaurant', 'venisia', 'parcelle', 'marchande', 'saf', 'bar', 'batrain', 'fi', 'sportiff', 'pamecas', 'cms', 'clando', 'daniel', 'sorano', 'hospitalier', 'aristide', 'dantec', 'difoncé', 'militaire', 'comercial', 'plateau', 'dialoré', 'plage', 'liberté', 'socio', 'culturel', 'wakhinane', 'nimzatt', 'claudel', \"d'epuration\", 'douta', 'baraka', 'oncf', 'demeure', 'caesar', 'stadium', 'marius', 'notre', 'dame', 'fastef', 'commissariat', 'dieuppeul', 'vdn', 'péage', 'mbenguéne', 'guinaw', 'rails', 'kennedy', 'kaki', 'institution', 'immaculée', 'conception', 'karack', 'ndiakhirate', 'sandicat', 'nabil', 'choucair', 'imprimerie', 'tandjan', 'sgbs', 'hotel', 'terrou', 'canal', 'dagoudane', 'ndoyéne', 'lébou', 'béthio', 'acapes', 'mbor', 'rouidate', 'thiane', 'ngogne', 'birane', 'ly', 'cosmetique', 'fallou', 'papa', 'guèye', 'tiléne', 'texaco', 'petersen', 'sodida', 'ndeureuhlou', 'club', 'ministre', 'khonkh', 'clean', 'fahd', 'ben', 'abdel', 'aziz', 'normale', 'supérieure', 'ningala', 'adji', 'niagna', 'dispensaire', 'norade', 'diamaguéne', 'maurice', 'delafosse', 'dominique', 'ministere', \"l'interieur\", 'senegalais', 'youssou', 'mbargane', 'ravin', 'sica', 'thiakhane', 'lat', 'flag', 'diarri', 'poul', 'medine', 'lazzare', 'privée', 'petits', 'génies', 'christa', 'talli', 'mourade', 'mbacké', 'mamelle', 'jet', \"d'eau\", 'awa', 'sante', 'gaspar', 'camara', 'cices', 'magic', 'land', \"l'obélisque\", 'saldia', 'mosquee', 'mbeur', 'abiya', 'kenedy', 'fu', 'mu', 'jëm', 'abebe', 'bikila', 'sengor', 'sofrac', 'pour', 'rokhaya', 'attijari', 'izdihar', 'bourguiba', 'poind', 'africatel', 'avs', 'diaakay', 'fouta', 'hôtel', 'diouma', 'leclerc', 'sotiba', 'hyacinthe', 'thiandoum', 'safco', 'caserne', 'samba', 'diéri', 'français', 'intersection', 'raby', 'equipe', 'plus', 'sur', 'mer', 'nioul', 'cge', '<UNK>']\n",
            "VERY STRANGE THAT THERE IS A c-cedi IN THE VOCABULARY EXTRACTED FROM THE TRAIN DATASET\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1_X0PHppglm",
        "outputId": "806a24da-d34c-4638-cc51-a1d89e071847"
      },
      "source": [
        "# vocabulary used by tokenizer (french alphabet)\n",
        "vocab_dict = {v for k, v in enumerate(tokenizer.get_vocab())}\n",
        "vocab_tokenizer = [v.lower() for v in vocab_dict]\n",
        "print(vocab_tokenizer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['m', \"'\", 'ë', 'r', 'v', 'ô', 'k', '<pad>', 'l', 'æ', 's', 'e', 'û', 'u', 'f', 'c', 'o', 'a', 'i', 'b', 'ù', 'â', 'ç', 'œ', 'w', 'd', 'ü', 'n', 'y', 't', 'î', 'q', '-', 'é', 'ï', 'è', 'j', '</s>', 'à', 'h', 'p', 'ê', '<s>', 'ÿ', 'g', 'z', 'x', '|', '<unk>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qvwRJ8l3Xiy"
      },
      "source": [
        "#### Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJIQfTGZ2_6w"
      },
      "source": [
        "# perplexity to acount for longer sequences\n",
        "def ngram_logprobability(sentence):\n",
        "    log_prob = 0\n",
        "    count = 0\n",
        "    for words in sentence:\n",
        "      for ngram in words:\n",
        "        # to avoid log(0) for unknown chars => many methods exist in the literature such as smoothing\n",
        "        # since log is monotonically increasing, adding a const should not change the ordering, right?\n",
        "        log_prob += np.log(LMmodel.score(ngram[1], [ngram[0]])+ 1e-8)\n",
        "        count += 1\n",
        "    return np.power(np.exp(log_prob), 1/count) # (inverse) perplexity to account for different word/ sentence length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LS2BR_kkOd7a"
      },
      "source": [
        "# WORD LEVEL LANGUAGE MODEL ################################################\n",
        "def ngram_logprobability(sentence):\n",
        "    log_prob = 0\n",
        "    count = 0\n",
        "    for ngram in sentence:\n",
        "      # to avoid log(0) for unknown chars => many methods exist in the literature such as smoothing\n",
        "      # since log is monotonically increasing, adding a const should not change the ordering, right?\n",
        "      log_prob += np.log(LMmodel.score(ngram[2], [ngram[0], ngram[1]])+ 1e-8)\n",
        "      count += 1\n",
        "    return np.power(np.exp(log_prob), 1/count) # (inverse) perplexity to account for different word/ sentence length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_tgrLoB2_31"
      },
      "source": [
        "def logprob_sentences(sentences):\n",
        "  '''\n",
        "  input: list of sentences\n",
        "  output: log probability for sentences\n",
        "  '''\n",
        "  # list with log probabilities\n",
        "  log_probs = len(sentences)* [-np.infty]\n",
        "\n",
        "  # creating list of sentences from string\n",
        "  list_sentences = [sentence.split(' ') for sentence in sentences]\n",
        "\n",
        "  # lower casing\n",
        "  for k in range(len(list_sentences)):\n",
        "    list_sentences[k] = [list(map(str.lower, [sent]))[0]\n",
        "                        for sent in list_sentences[k]]\n",
        "\n",
        "  # list(sentence_list(word_list(ngrams)))\n",
        "  list_ngrams = [[list(ngrams(pad_both_ends(word, n=2), n=2)) for word in sentence] for sentence in list_sentences]\n",
        "\n",
        "  for k, sentence in enumerate(list_ngrams):\n",
        "    log_probs[k] = ngram_logprobability(sentence)\n",
        "\n",
        "  return log_probs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWyFoNTLOn6o"
      },
      "source": [
        "# WORD LEVEL LANGUAGE MODEL ################################################\n",
        "def logprob_sentences(sentences):\n",
        "  '''\n",
        "  input: list of sentences\n",
        "  output: log probability for sentences\n",
        "  '''\n",
        "\n",
        "  # list with log probabilities\n",
        "  log_probs = len(sentences)* [-np.infty]\n",
        "\n",
        "  # creating list of sentences from string\n",
        "  list_sentences = [sentence.split(' ') for sentence in sentences]\n",
        "\n",
        "  # lower casing\n",
        "  for k in range(len(list_sentences)):\n",
        "    list_sentences[k] = [word.lower() for word in list_sentences[k]]\n",
        "\n",
        "  # list(sentence_list(word_list(ngrams)))\n",
        "  list_ngrams = [list(ngrams(pad_both_ends(sentence, n=3), n=3)) for sentence in list_sentences]\n",
        "\n",
        "  for k, sentence in enumerate(list_ngrams):\n",
        "    log_probs[k] = ngram_logprobability(sentence)\n",
        "\n",
        "  return log_probs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tBUrQob5MWK"
      },
      "source": [
        "## Unit Tests"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTHSNpfq88_H",
        "outputId": "3ff44c4b-d24f-4b75-a71d-62ba40af8d8a"
      },
      "source": [
        "df_lm['transcription'].values"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Rufsac', 'Pharmacie Talibou Dabo', 'Avenue Faidherbe',\n",
              "       'Cité mére Thérésa', 'Gare de Thiaroye'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZ8j_BQvUQUd",
        "outputId": "13cbfa45-72a3-4930-c67f-94e9600e5eae"
      },
      "source": [
        "LMmodel.score('e', ['l', 'y', 'c'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2CjLTGP2_vm",
        "outputId": "1b8499cc-8822-4ac7-9af5-798a444863c5"
      },
      "source": [
        "print(logprob_sentences(['lycée', 'lycee', 'lycèe']))\n",
        "print(logprob_sentences(['lycée camp', 'lycee camp', 'lycèe camp']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[9.371477854353765e-07, 7.657042980709192e-07, 9.999999999999994e-09]\n",
            "[1.8303355454405124e-05, 2.5884859598092657e-07, 9.999999999999982e-09]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3AZaOoNn3ydn",
        "outputId": "f4d4c5c8-e9bf-4a33-cf6d-78e2b86460e7"
      },
      "source": [
        "print(logprob_sentences(['keur', 'keur', 'keurr']))\n",
        "print(logprob_sentences(['keur massar', 'keur masar', 'keurr massar']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2.654593871608607e-07, 2.654593871608607e-07, 9.999999999999994e-09]\n",
            "[0.0011694955341792163, 1.1694955283317372e-07, 1.0000000024999989e-06]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yQsqSimClN_"
      },
      "source": [
        "#\n",
        "input_dict = df_valid['audio_signal'].apply(prepare_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LW2SBQ8IEyrE"
      },
      "source": [
        "model_lm_valid = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81zQn2aG1EVU",
        "outputId": "21b23bbe-54d9-48b6-8c60-e40dce8b7dd7"
      },
      "source": [
        "# ATTENTION: XLSR model seems to be overconfident -> places all probability mass on one logit\n",
        "from operator import itemgetter\n",
        "\n",
        "## Predictions\n",
        "nbeams = 80\n",
        "\n",
        "# word error rate\n",
        "wer_ = []\n",
        "\n",
        "#\n",
        "softmax = nn.Softmax(dim=2)\n",
        "\n",
        "## WER over everything (one long string)\n",
        "label_str = ''\n",
        "pred_str = ''\n",
        "\n",
        "for idx in range(len(df_valid)):\n",
        "  #print('-----------------')\n",
        "  logits = XLSRmodel(input_dict.values[idx].input_values.to(\"cuda\")).logits\n",
        "  # sum_j(output_ij) = 1 where i is column and j is row\n",
        "  output = softmax(logits) # logits -> probabilities\n",
        "\n",
        "  # beam search\n",
        "  beams_int = beam_search_decoder(torch.squeeze(output).tolist(), top_k = nbeams) # beams\n",
        "  beams_str = nbeams*['']\n",
        "\n",
        "  for k in range(nbeams):\n",
        "    pred_ids, pred_prob = beams_int[k]\n",
        "    beams_str[k] = processor.decode(pred_ids)\n",
        "\n",
        "  #logprob = logprob_sentences(beams_str)\n",
        "  #pred_str = beams_str[np.argmax(logprob)]\n",
        "\n",
        "  # prediction P(final) = Alpha * P(model) + Beta * P(L.M.)\n",
        "  ## WER over everything (one long string)\n",
        "  \n",
        "  pred = []\n",
        "  for k, word in enumerate(beams_str[np.argmax(logprob_sentences(beams_str))].split(' ')):\n",
        "    sim = difflib.get_close_matches(word, xv, n=3, cutoff=0.8)\n",
        "\n",
        "    if sim == []:\n",
        "      sim = word\n",
        "    else:\n",
        "      sim = sim[0]\n",
        "\n",
        "    pred.append(sim)\n",
        "\n",
        "  pred_str+= ' '.join(pred)+ ' '\n",
        "  \n",
        "\n",
        "  '''\n",
        "  # prediction P(final) = Alpha * P(model) + Beta * P(L.M.)\n",
        "  # loop through XLSR output (in char space or in token space?)\n",
        "  # char by char join probability distributions\n",
        "  # -> this can only locally improve the prediction \n",
        "  # -> it's not possible to add new words / split words\n",
        "  # -> alignment problems arise\n",
        "  # -> difficulty CTC output: characters can be repeated\n",
        "  # https://arxiv.org/pdf/1612.02695.pdf\n",
        "\n",
        "  pred_ids = torch.argmax(logits, dim=-1)[0]\n",
        "  pred = []\n",
        "\n",
        "  # keep track of position in sentence\n",
        "  pos = []\n",
        "  chars = []\n",
        "\n",
        "  for k, tok in enumerate(pred_ids):\n",
        "    if tok == 4 and len(chars)> 0:\n",
        "      # probability of char sequence\n",
        "      XLSR_prob = np.exp(sum(np.log(output.to('cpu').detach().numpy()[0][pos, chars])))\n",
        "      LM_prob = ngram_logprobability([' '.join(llist[iter:iter+3])])\n",
        "    \n",
        "      # weighting of prediction\n",
        "      pred.append(processor.decode(chars))\n",
        "\n",
        "      # reset word when observing space\n",
        "      chars = []\n",
        "      pos = []\n",
        "\n",
        "    if tok != 4:\n",
        "      # fill word with characters\n",
        "      pos.append(k)\n",
        "      chars.append(tok.to('cpu').item())\n",
        "\n",
        "  pred_str += ' '.join(pred) + ' '\n",
        "  '''\n",
        "\n",
        "  ####\n",
        "  #logprob_sentences(sentences):\n",
        "  #print(pred_ids)\n",
        "  #print(processor.decode(pred_ids))\n",
        "\n",
        "  #beams_str[k] = processor.decode(pred_ids)\n",
        "\n",
        "  ####\n",
        "  model_lm_valid.append(' '.join(pred))\n",
        "  \n",
        "  label_str+= df_valid[\"transcription\"].values[idx].lower()+ ' '\n",
        "\n",
        "wer_.append(wer_metric.compute(predictions=[pred_str], references=[label_str]))\n",
        "\n",
        "print(np.mean(wer_))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.03421607686899461\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "oRL11nfPu47t",
        "outputId": "4b096786-aeec-4861-93b9-2b86709db836"
      },
      "source": [
        "N = 20: 0.03655964377782986\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-133-9cb135a030ef>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    N = 20: 0.03655964377782986\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGFgu066G2Ew"
      },
      "source": [
        "'''\n",
        "### weighting of prediction of language model vs XLSR model\n",
        "  # prediction P(final) = Alpha * P(model) + Beta * P(L.M.)\n",
        "  pred_ids = torch.argmax(logits, dim=-1)[0]\n",
        "  pred = []\n",
        "\n",
        "  # keep track of position in sentence\n",
        "  pos = []\n",
        "  chars = []\n",
        "  iter = 0\n",
        "\n",
        "  # get best beam from beam search\n",
        "  logprob = logprob_sentences(beams_str)\n",
        "  beam = beams_str[np.argmax(logprob)]\n",
        "  llist = beam.split(' ')\n",
        "  llist.insert(0, '<pad>')\n",
        "  llist.insert(0, '<pad>')\n",
        "\n",
        "  for k, tok in enumerate(pred_ids):\n",
        "    if tok == 4 and len(chars)> 0:\n",
        "      # probability of word\n",
        "      XLSR_prob = np.exp(sum(np.log(output.to('cpu').detach().numpy()[0][pos, chars])))\n",
        "      LM_prob = ngram_logprobability([' '.join(llist[iter:iter+3])])\n",
        "    \n",
        "      # weighting of prediction\n",
        "      if np.log(XLSR_prob) > 0.01* np.log(LM_prob):\n",
        "        pred.append(processor.decode(chars))\n",
        "      else:\n",
        "        try:\n",
        "          pred.append(llist[iter+2])\n",
        "        except:\n",
        "          print(llist)\n",
        "          print(pred)\n",
        "          print(iter)\n",
        "\n",
        "      # reset word when observing space\n",
        "      chars = []\n",
        "      pos = []\n",
        "      iter += 1\n",
        "\n",
        "    if tok != 4:\n",
        "      # fill word with characters\n",
        "      pos.append(k)\n",
        "      chars.append(tok.to('cpu').item())\n",
        "\n",
        "  pred_str += ' '.join(pred) + ' '\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-J0FsSws7xHY"
      },
      "source": [
        "for k in range(len(df_valid)):\n",
        "  df_valid['transcription'].values[k] = df_valid['transcription'].values[k].lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_eRmke60FAb2",
        "outputId": "8ccc183a-67ff-4963-fa58-4be8e1861020"
      },
      "source": [
        "for k in range(len(model_valid)):\n",
        "  if df_valid['transcription'].values[k] != model_lm_valid[k]:\n",
        "    print('-----------')\n",
        "    #print(k)\n",
        "    #print(model_valid[k])\n",
        "    #print(model_vocab_valid[k])\n",
        "    print(model_lm_valid[k])\n",
        "    print(df_valid['transcription'].values[k])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------\n",
            "ban car mooy dem cheikh ahmadou bando mackétaboune<unk> dakar\n",
            "ban car mooy dem cheikh ahmadou bamba mbacke avenue, dakar\n",
            "-----------\n",
            "croisement telry diallo\n",
            "croisement tally diallo\n",
            "-----------\n",
            "scat - urbam\n",
            "scat urbam\n",
            "-----------\n",
            "gare ferroviaire de dakar\n",
            "gare feroviére de dakar\n",
            "-----------\n",
            "ecole sherif youssou thiaw laye malika\n",
            "ecole sherif ouseynou thiaw laye malicka\n",
            "-----------\n",
            "cité poste thiaroye laa bëgg dem\n",
            "cinéma poste thiaroye laa bëgg dem\n",
            "-----------\n",
            "kat - urbam\n",
            "scat - urbam\n",
            "-----------\n",
            "ministère de la santé de la prévention\n",
            "ministère de la santé et de la prévention\n",
            "-----------\n",
            "tali bou khonkh <unk>station clean oil<unk>\n",
            "tali bou khonkh (station clean oil)\n",
            "-----------\n",
            "chambre de pamers\n",
            "chambre de commerce\n",
            "-----------\n",
            "ouakam cite assemblée nationale\n",
            "ouakam cité assemblée nationale\n",
            "-----------\n",
            "ministere de l'interieur senegal\n",
            "ministere de l'interieur senegalais\n",
            "-----------\n",
            "stade de baol diop\n",
            "stade demba diop\n",
            "-----------\n",
            "seydina limamoulaye avenue<unk> dakar\n",
            "seydina limamoulaye avenue, dakar\n",
            "-----------\n",
            "pharmacie serigne bamba mbacké colobane\n",
            "pharmacie serigne bamba m'backé colobane\n",
            "-----------\n",
            "pikine saf mar\n",
            "pikine saf bar\n",
            "-----------\n",
            "boulevard général de gaule (centenaire)\n",
            "bd général de gaule (centenaire)\n",
            "-----------\n",
            "equipus\n",
            "equipe plus\n",
            "-----------\n",
            "pharmacie marie\n",
            "pharmacie mamelle\n",
            "-----------\n",
            "marché grande médina\n",
            "marché grand médine\n",
            "-----------\n",
            "elton fks usine de farine\n",
            "elton - fks usine de farine\n",
            "-----------\n",
            "brioche dorée majuises\n",
            "brioche dorée maristes\n",
            "-----------\n",
            "yavuz sédim- collège bosphore\n",
            "yavuz sélim - collège bosphore\n",
            "-----------\n",
            "gare feroviére de dakar\n",
            "gare ferroviaire de dakar\n",
            "-----------\n",
            "mosquée de mamelles\n",
            "mosquée des mamelles\n",
            "-----------\n",
            "terminus parcelles terminus parcelles assainies\n",
            "terminus parcelles assainies\n",
            "-----------\n",
            "cheikh anta diop avenue<unk> dakar\n",
            "cheikh anta diop avenue, dakar\n",
            "-----------\n",
            "route de fann dakar\n",
            "route de fann, dakar\n",
            "-----------\n",
            "bd général de gaule sanitaire\n",
            "bd général de gaule (centenaire)\n",
            "-----------\n",
            "samerie ouakam\n",
            "gendarmerie ouakam\n",
            "-----------\n",
            "centre de souvedade\n",
            "centre de sauvegarde\n",
            "-----------\n",
            "hapijari izdihar\n",
            "attijari izdihar\n",
            "-----------\n",
            "universite dakar bourguiba dem\n",
            "université dakar bourguiba\n",
            "-----------\n",
            "samu municipale grand yoff\n",
            "samu municipale grand-yoff\n",
            "-----------\n",
            "dame laay jële oto bide ngir dem\n",
            "ndax mën naa jël oto bi ngir dem\n",
            "-----------\n",
            "radio futurs media <unk>rsn\n",
            "radio futurs media (rfm)\n",
            "-----------\n",
            "peulh pompier de rufisque\n",
            "sapeur pompier de rufisque\n",
            "-----------\n",
            "gare ancf\n",
            "gare oncf\n",
            "-----------\n",
            "rond point de grand mbao\n",
            "rond point grand mbao\n",
            "-----------\n",
            "allées papa guèye fann\n",
            "allées papa guèye fall\n",
            "-----------\n",
            "ministère de la santé de la prévention\n",
            "ministère de la santé et de la prévention\n",
            "-----------\n",
            "benn dakar\n",
            "bem dakar\n",
            "-----------\n",
            "stadium maurice ndiaye\n",
            "stadium marius ndiaye\n",
            "-----------\n",
            "garage camion fan vidange\n",
            "garage camion vidange\n",
            "-----------\n",
            "centre hospitalier universite aristide le dantec\n",
            "centre hospitalier universitaire aristide le dantec\n",
            "-----------\n",
            "pharmacie serigne bamba mbacké colobane\n",
            "pharmacie serigne bamba m'backé colobane\n",
            "-----------\n",
            "taly serigne mourade mbacké\n",
            "talli serigne mourade mbacké\n",
            "-----------\n",
            "flad diarri poul\n",
            "flag diarri poul\n",
            "-----------\n",
            "lycée kennedy par dakar dem dikk\n",
            "lycée kenedy par dakar dem dikk\n",
            "-----------\n",
            "avenue du roi fahd ben abdel aziz<unk> dakar\n",
            "avenue du roi fahd ben abdel aziz, dakar\n",
            "-----------\n",
            "terminus leclair\n",
            "terminus leclerc\n",
            "-----------\n",
            "arret port\n",
            "aeroport\n",
            "-----------\n",
            "poste diouma rufisque\n",
            "poste courant rufisque\n",
            "-----------\n",
            "croisement total ouakam\n",
            "coisement total ouakam\n",
            "-----------\n",
            "route de aéroport\n",
            "route de l'aéroport\n",
            "-----------\n",
            "taly serigne mourade mbacké\n",
            "talli serigne mourade mbacké\n",
            "-----------\n",
            "fasto rufisque\n",
            "place de l'obelisque\n",
            "-----------\n",
            "centre de sauve gare\n",
            "centre de sauvegarde\n",
            "-----------\n",
            "gare ferroviaire de dakar\n",
            "gare feroviére de dakar\n",
            "-----------\n",
            "cité capexe\n",
            "cité capec\n",
            "-----------\n",
            "pharmacie akoulrap\n",
            "pharmacie niakoul rap\n",
            "-----------\n",
            "ban car mooy dem clinique du cav manuel\n",
            "ban car mooy dem clinique du cap manuel\n",
            "-----------\n",
            "seydina limamoulaye avenue<unk> dakar\n",
            "seydina limamoulaye avenue, dakar\n",
            "-----------\n",
            "cité bissane\n",
            "cité bissap\n",
            "-----------\n",
            "rond poind sipres l azhar\n",
            "rond poind sipres al azhar\n",
            "-----------\n",
            "ecole shirike ouseynou thiaw laye malicka\n",
            "ecole sherif ouseynou thiaw laye malicka\n",
            "-----------\n",
            "virage des art\n",
            "village des art\n",
            "-----------\n",
            "rond point goul gui penitance\n",
            "rond point gouy gui penitance\n",
            "-----------\n",
            "route de fann dakar\n",
            "route de fann, dakar\n",
            "-----------\n",
            "ban car mooy dem rond point zone bache\n",
            "ban car mooy dem rond point zone b\n",
            "-----------\n",
            "sodida boune\n",
            "sodida\n",
            "-----------\n",
            "avenue du roi fahd bénadel aziz<unk> dakar\n",
            "avenue du roi fahd ben abdel aziz, dakar\n",
            "-----------\n",
            "stade alassane tiko\n",
            "stade alassane djigo\n",
            "-----------\n",
            "<unk>rond point ouakam  x <unk>mermoz <unk>\n",
            "( rond point ouakam ) x (mermoz )\n",
            "-----------\n",
            "dakhmu laay jële oto by ngir dem\n",
            "ndax mën naa jël oto bi ngir dem\n",
            "-----------\n",
            "cheikh anta diop avenue<unk> dakar\n",
            "cheikh anta diop avenue, dakar\n",
            "-----------\n",
            "ban car mooy dem cheikh ahmadou bamba mbacke avenue<unk> dakar\n",
            "ban car mooy dem cheikh ahmadou bamba mbacke avenue, dakar\n",
            "-----------\n",
            "ecole sheikhif sékou thiaw laye malicka\n",
            "ecole sherif ouseynou thiaw laye malicka\n",
            "-----------\n",
            "teukho\n",
            "thokho\n",
            "-----------\n",
            "route malika\n",
            "route de malika\n",
            "-----------\n",
            "tournalou bou\n",
            "tournalou boune\n",
            "-----------\n",
            "lycée thierno ouseynou tall\n",
            "lycée thierno seydou nourou tall\n",
            "-----------\n",
            "marché sandicat texaco\n",
            "marché sandicat ( texaco )\n",
            "-----------\n",
            "dama naa jële oto bi ngir dem\n",
            "ndax mën naa jël oto bi ngir dem\n",
            "-----------\n",
            "avenue du roi fahd ben abdel aziz<unk> dakar\n",
            "avenue du roi fahd ben abdel aziz, dakar\n",
            "-----------\n",
            "croisement ben\n",
            "croisement béthio\n",
            "-----------\n",
            "diaakay cité sonatel\n",
            "diakayy cité sonatel\n",
            "-----------\n",
            "terkhin <unk>fleuve<unk> de mbao\n",
            "dékh (fleuve) de mbao\n",
            "-----------\n",
            "stade galandou diouf de rufisque\n",
            "stade nganladou diouf de rufisque\n",
            "-----------\n",
            "rue liman fall\n",
            "rue elimanel fall\n",
            "-----------\n",
            "gare feroviére de dakar\n",
            "gare ferroviaire de dakar\n",
            "-----------\n",
            "tally serigne mourade mbacké\n",
            "talli serigne mourade mbacké\n",
            "-----------\n",
            "ministere de l'interieur senérél\n",
            "ministere de l'interieur senegalais\n",
            "-----------\n",
            "universite amadou hampate ban\n",
            "universite amadou hampate ba\n",
            "-----------\n",
            "pharmacie rompel santhiaba\n",
            "pharmacie rond point santhiaba\n",
            "-----------\n",
            "ban oto mooy jaay guinaw rails\n",
            "ban oto mooy jaar guinaw rails\n",
            "-----------\n",
            "bus bi for jëm\n",
            "bus bi fu mu jëm\n",
            "-----------\n",
            "cheikh anta diop avenue<unk> dakar\n",
            "cheikh anta diop avenue, dakar\n",
            "-----------\n",
            "stade iba mbaye diop\n",
            "stade iba mar diop\n",
            "-----------\n",
            "rond point dan théâtre national de dakar\n",
            "rond point grand théâtre national de dakar\n",
            "-----------\n",
            "croisement diassay\n",
            "croisement diaxxay\n",
            "-----------\n",
            "gare ferroviaire de dakar\n",
            "gare feroviére de dakar\n",
            "-----------\n",
            "bus bi fu mooy jëm\n",
            "bus bi fu mu jëm\n",
            "-----------\n",
            "bargny bargny diop\n",
            "bargny diop\n",
            "-----------\n",
            "route de fann<unk> dakar\n",
            "route de fann, dakar\n",
            "-----------\n",
            "police de guédiawaye\n",
            "police guédiawaye\n",
            "-----------\n",
            "cambutaire ouakam\n",
            "camp militaire ouakam\n",
            "-----------\n",
            "n laay def ngir dem gendarmerie pikine\n",
            "nan laay def ngir dem gendarmerie pikine\n",
            "-----------\n",
            "cité djily mballo\n",
            "cité djily m'baye\n",
            "-----------\n",
            "seydina limamoulaye avenue<unk> dakar\n",
            "seydina limamoulaye avenue, dakar\n",
            "-----------\n",
            "ndxmu nan jële oto bi ngir dem\n",
            "ndax mën naa jël oto bi ngir dem\n",
            "-----------\n",
            "avenue du roi fahh ben abdel aziz<unk> dakar\n",
            "avenue du roi fahd ben abdel aziz, dakar\n",
            "-----------\n",
            "avenue du roi fb ben abdel azir<unk> dakar\n",
            "avenue du roi fahd ben abdel aziz, dakar\n",
            "-----------\n",
            "samu municipale grand yoff\n",
            "samu municipale grand-yoff\n",
            "-----------\n",
            "ban car mooy dem cheikh badou bamba mbacké avenue<unk> dakar\n",
            "ban car mooy dem cheikh ahmadou bamba mbacke avenue, dakar\n",
            "-----------\n",
            "route de fann dakar\n",
            "route de fann, dakar\n",
            "-----------\n",
            "supdeco point campus point e\n",
            "supdeco campus point e\n",
            "-----------\n",
            "nan laay di ba dem tally basse\n",
            "nan laay def ba dem tally bou bess\n",
            "-----------\n",
            "stade galandou diouf de rufisque\n",
            "stade nganladou diouf de rufisque\n",
            "-----------\n",
            "sipres antl lazzare\n",
            "sipres al azhar\n",
            "-----------\n",
            "enil sorano dakar\n",
            "daniel sorano dakar\n",
            "-----------\n",
            "brioche dorée guediawaye\n",
            "brioche doré guédiawaye\n",
            "-----------\n",
            "pharmacie serigne bamba mbacke colobane\n",
            "pharmacie serigne bamba m'backé colobane\n",
            "-----------\n",
            "ban car mooy dem cheikh ahmadou ba mbacke avenue<unk> dakar\n",
            "ban car mooy dem cheikh ahmadou bamba mbacke avenue, dakar\n",
            "-----------\n",
            "cimapost thiaroye\n",
            "cinéma poste thiaroye\n",
            "-----------\n",
            "yeumbeul bus\n",
            "yeumbeul boune\n",
            "-----------\n",
            "tre de transfusion sanguine\n",
            "centre de transfusion sanguine\n",
            "-----------\n",
            "avenue de fann\n",
            "avenue des far\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5P_LdfJu-BjU",
        "outputId": "38435c13-10bc-42a7-945a-8d658d321c6e"
      },
      "source": [
        "df_valid.values[894]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['edc05ac2cc903c79d443bbb0305b2824a980e0097b3d5c64711402a28620d17d909fa30a831682166fad2c4bc16aaa49e8ed493f5d53097530c970104226d605',\n",
              "       2, 0, 'twenties', 'female', 'marché sandicat ( texaco )',\n",
              "       array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCplyao15gDu"
      },
      "source": [
        "## spell checking\n",
        "# https://web.stanford.edu/~jurafsky/slp3/B.pdf\n",
        "# 1) train LM as n-gram. prediction with model. run prediction through levenstein distance, use n-gram to vote\n",
        "# 2) use pystellchecker -> only in French so far\n",
        "# https://pypi.org/project/pyspellchecker/\n",
        "# 3) textBlob \n",
        "# https://stackabuse.com/spelling-correction-in-python-with-textblob/\n",
        "# https://github.com/sloria/TextBlob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYfPuOkg8izK",
        "outputId": "0392bde5-c806-4cb2-b488-f7def3e7b814"
      },
      "source": [
        "id= 'a8d8c6221854d1b721162a4ecfbdf87554e0b39c782ccd1914aeaddf3491a92df99ac7cee4264d5b031b0e779e1e64d7206deca98ea39009e579fb7cab164ffe'\n",
        "\n",
        "input_dict = df_valid[df_valid['ID']==id]['audio_signal'].apply(prepare_dataset)\n",
        "logits = XLSRmodel(input_dict.values[0].input_values.to(\"cuda\")).logits\n",
        "pred_ids = torch.argmax(logits, dim=-1)[0]\n",
        "\n",
        "print(processor.decode(pred_ids))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4390    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
            "Name: audio_signal, dtype: object\n",
            "[{'input_values': tensor([[-7.9619e-05, -7.9619e-05, -7.9619e-05,  ..., -7.9619e-05,\n",
            "         -7.9619e-05, -7.9619e-05]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]])}]\n",
            "sheikh anta diop avenue<unk> dakar\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9j6e3XwuSyhj"
      },
      "source": [
        "# with word 3-gram\n",
        "N=3 -> 0.07475978439184439\n",
        "N=5 -> 0.07030700726505741\n",
        "N=10 -> 0.06304194984766816\n",
        "N=50 -> 0.05319896883056011 => 0.118 = 11.8% auf test satz\n",
        "\n",
        "# with word 2-gram\n",
        "N=3 -> 0.07499414108272791\n",
        "N=5 -> 0.07077572064682447\n",
        "N=10 -> 0.06397937661120225\n",
        "N=20 -> 0.059057886102648234\n",
        "\n",
        "# with word 1-gram\n",
        "N=3 -> 0.07710335130067963\n",
        "N=5 -> 0.07499414108272791\n",
        "N=10 -> 0.07382235762831028"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkcHVqNAm6Fp"
      },
      "source": [
        "# with char 5-gram\n",
        "N=3 -> 0.07780642137333021\n",
        "N=5 -> 0.07569721115537849 down\n",
        "N=10 ->0.07405671431919382 down\n",
        "\n",
        "# with char 4-gram\n",
        "N=3 -> 0.07921256151863136\n",
        "N=5 -> 0.0787438481368643 down\n",
        "N=10 ->0.07850949144598078 down\n",
        "\n",
        "# with char 3-gram\n",
        "N=1 -> 0.0824935551910007\n",
        "N=2 -> 0.08202484180923365 down\n",
        "N=3 -> 0.08366533864541832 up\n",
        "N=5 -> 0.08577454886337005 up\n",
        "N=80 ->0.14811342863838764 up\n",
        "\n",
        "# with char 2-gram\n",
        "N=3 -> 0.08600890555425357\n",
        "N=5 -> 0.09210217951722521 up\n",
        "\n",
        "# w/o\n",
        "    -> 0.0824935551910007"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtWwOM_31G1E"
      },
      "source": [
        "### Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cAyWHup1IGk"
      },
      "source": [
        "## prediction\n",
        "# load data (dataframe) -> empty entries\n",
        "df_test = pd.read_feather('drive/MyDrive/Colab Notebooks/data/ASR_test_audio1564.ft')\n",
        "df_test = df_test[['ID', 'audio_signal']]\n",
        "\n",
        "df_test.head()\n",
        "\n",
        "def prepare_dataset(batch):\n",
        "    return processor(batch, return_tensors=\"pt\", sampling_rate=16*1e3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWVuQMO9WoWL"
      },
      "source": [
        "#\n",
        "input_dict = df_test['audio_signal'].apply(prepare_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4tNbrEyNLnJ",
        "outputId": "b831dbd9-818a-4ac9-c4b5-2c63e6e0028f"
      },
      "source": [
        "# run through processor\n",
        "import difflib\n",
        "#input_dict = df_test['audio_signal'].apply(prepare_dataset)\n",
        "preds = []\n",
        "\n",
        "nbeams = 80\n",
        "\n",
        "#\n",
        "softmax = nn.Softmax(dim=2)\n",
        "\n",
        "# run through model and decoder\n",
        "for i in range(len(df_test)):\n",
        "  logits = XLSRmodel(input_dict.values[i].input_values.to('cuda')).logits\n",
        "  output = softmax(logits) # logits -> probabilities\n",
        "  \n",
        "  # beam search\n",
        "  beams_int = beam_search_decoder(torch.squeeze(output).tolist(), top_k = nbeams) # beams\n",
        "  beams_str = nbeams*['']\n",
        "\n",
        "  for k in range(nbeams):\n",
        "    pred_ids, pred_prob = beams_int[k]\n",
        "    beams_str[k] = processor.decode(pred_ids)\n",
        "\n",
        "  # prediction P(final) = Alpha * P(model) + Beta * P(L.M.)\n",
        "  logprob = logprob_sentences(beams_str)\n",
        "  pred_str = beams_str[np.argmax(logprob)]\n",
        "\n",
        "  pred = []\n",
        "  for word in pred_str.split(' '):\n",
        "    sim = difflib.get_close_matches(word, xv, n=1)\n",
        "\n",
        "    if sim == []:\n",
        "      sim = word\n",
        "    else:\n",
        "      sim = sim[0]\n",
        "\n",
        "    pred.append(sim)\n",
        "\n",
        "  preds.append(' '.join(pred))\n",
        "\n",
        "  if i % 200 == 0:\n",
        "    print('Sentence '+ str(i))\n",
        "\n",
        "# save as csv\n",
        "dfpred = pd.DataFrame(list(zip(list(df_test['ID'].values), preds)), columns=['ID', 'transcription'])\n",
        "dfpred.to_csv('./drive/MyDrive/Colab Notebooks/predictionsLM_24AprLMvocab.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence 0\n",
            "Sentence 200\n",
            "Sentence 400\n",
            "Sentence 600\n",
            "Sentence 800\n",
            "Sentence 1000\n",
            "Sentence 1200\n",
            "Sentence 1400\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNQXxwR5NuGC"
      },
      "source": [
        "from nltk.probability import FreqDist\n",
        "from wordcloud import WordCloud, ImageColorGenerator\n",
        "\n",
        "#\n",
        "words = df['transcription']\n",
        "allwords = []\n",
        "\n",
        "for wordlist in words:\n",
        "  allwords += list(wordlist.lower().split())\n",
        "\n",
        "# histogram\n",
        "mostcommon_small = FreqDist(allwords).most_common(10000)\n",
        "xv, yv = zip(*mostcommon_small)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cazQLYfATOr6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}